{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn import linear_model, datasets\n",
    "import os\n",
    "import simplejson\n",
    "import sys\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_svmlight_file\n",
    "\n",
    "import loss_functions, make_plots\n",
    "import scr, tr, sgd, saga, ns\n",
    "from sketches import gaussian, srht, less, sparse_rademacher, rrs, sjlt\n",
    "\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) load data and initialize weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset a9a loaded\n",
      "n = 32561 d = 123\n"
     ]
    }
   ],
   "source": [
    "#####################\n",
    "#### Load  Data  ####\n",
    "#####################\n",
    "\n",
    "dataset_name='a9a' \n",
    "\n",
    "#logistic regression:\n",
    "if dataset_name=='a9a':\n",
    "    X, Y = load_svmlight_file('data/a9a')\n",
    "    X = X.toarray()\n",
    "    Y= [0 if e == -1 else e for e in Y]\n",
    "    Y=np.array(Y)      \n",
    "    d = X.shape[1]\n",
    "    n = X.shape[0]\n",
    "    w = np.zeros(d)\n",
    "    \n",
    "#multinominal (softmax) regression:\n",
    "elif dataset_name == 'mnist':\n",
    "    import scipy \n",
    "    X, Y = load_svmlight_file('data/mnist')\n",
    "    X = X.toarray()\n",
    "    nC = len(np.unique(Y))    \n",
    "    ## one-hot encoding of labels\n",
    "    data   = np.ones(len(Y))\n",
    "    indptr = np.arange(len(Y)+1)\n",
    "    ground_truth = scipy.sparse.csr_matrix((data, Y, indptr))\n",
    "    Y = ground_truth.todense() #gives a matrix with [Ground_Truth]i,j: is sample i in class j? (n x nC)\n",
    "    d = X.shape[1]\n",
    "    n = X.shape[0]\n",
    "    w = np.zeros(nC*d)\n",
    "\n",
    "#general function    \n",
    "elif dataset_name=='rosenbrock':\n",
    "    d=2\n",
    "    n=1 \n",
    "    w = np.zeros(d)\n",
    "\n",
    "print ('Dataset', dataset_name, 'loaded')    \n",
    "print ('n = ' + str(n) + ' d = ' + str(d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c) specify a loss, gradient, Hessian-vector-product and Hessian computations\n",
    "- (latter only needed for hard case)\n",
    "- functions need to have (w,X,Y,kwargs...) as input structure !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if dataset_name in {'a9a'}:\n",
    "    loss_computation = loss_functions.logistic_loss\n",
    "    gradient_computation = loss_functions.logistic_loss_gradient\n",
    "    hessian_vector_computation = loss_functions.logistic_loss_Hv\n",
    "    hessian_computation = loss_functions.logistic_loss_hessian\n",
    "    \n",
    "    # Additional arguments that are to be passed to the Loss, Gradient, etc. computations\n",
    "    loss_args= {}\n",
    "    loss_args['alpha'] = 1e-3 # regularizer\n",
    "\n",
    "if dataset_name in {'mnist'}:\n",
    "    loss_computation = loss_functions.softmax_loss\n",
    "    gradient_computation = loss_functions.softmax_loss_gradient\n",
    "    hessian_vector_computation = loss_functions.softmax_loss_Hv\n",
    "    hessian_computation = loss_functions.softmax_loss_hessian\n",
    "    \n",
    "    loss_args= {}\n",
    "    loss_args['alpha'] = 1e-3\n",
    "    loss_args['n_classes'] = nC # for multiclass (softmax) regression\n",
    "\n",
    "    \n",
    "elif dataset_name == 'rosenbrock':\n",
    "    loss_computation = loss_functions.rosenbrock_loss\n",
    "    gradient_computation = loss_functions.rosenbrock_gradient\n",
    "    hessian_vector_computation = loss_functions.rosenbrock_Hv\n",
    "    hessian_computation = loss_functions.rosenbrock_hessian\n",
    "    \n",
    "    loss_args= {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def show_plots(x_limits_time=None):\n",
    "    list_params=[]\n",
    "    list_loss=[]\n",
    "    list_x=[]\n",
    "    list_samples=[]\n",
    "    \n",
    "    log_scale=True\n",
    "    \n",
    "    over_time=True\n",
    "    over_iterations=True\n",
    "    over_epochs=True\n",
    "\n",
    "    if NS:\n",
    "        list_loss.append(NS_loss)\n",
    "        list_x.append(NS_x)\n",
    "        list_samples.append(NS_samples)\n",
    "        list_params.append('NS')\n",
    "\n",
    "    if SCR:\n",
    "        list_loss.append(SCR_loss)\n",
    "        list_x.append(SCR_x)\n",
    "        list_samples.append(SCR_samples)\n",
    "        list_params.append('SCR')\n",
    "        \n",
    "    if TR:\n",
    "        list_loss.append(TR_loss)\n",
    "        list_x.append(TR_x)\n",
    "        list_samples.append(TR_samples)\n",
    "        list_params.append('TR')\n",
    "        \n",
    "    if SGD:\n",
    "        list_loss.append(SGD_loss)\n",
    "        list_x.append(SGD_x)\n",
    "        list_samples.append(SGD_samples)\n",
    "        list_params.append('SGD')\n",
    "    if SAGA:\n",
    "        list_loss.append(SAGA_loss)\n",
    "        list_x.append(SAGA_x)\n",
    "        list_samples.append(SAGA_samples)\n",
    "        list_params.append('SAGA')\n",
    "        \n",
    "    if over_time:\n",
    "        make_plots.two_d_plot_time(list_loss,list_x,list_params,dataset_name, n, d, log_scale,x_limits=x_limits_time)\n",
    "        \n",
    "    if over_iterations:\n",
    "        make_plots.two_d_plot_iterations(list_loss,list_x,list_params,dataset_name, n, d, log_scale)\n",
    "        \n",
    "    if over_epochs:\n",
    "        make_plots.two_d_plot_epochs(list_loss,list_samples,list_params,dataset_name, n, d, log_scale)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d) Set parameters and run methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Newton Sketch ---\n",
      "\n",
      "- Subproblem_solver: lanczos\n",
      "- Hessian_sampling: True\n",
      "- Gradient_sampling: False\n",
      "- Sampling_scheme: adaptive \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\EECS 598\\cubic\\ns.py:201: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  v = np.linalg.lstsq(hessian, -grad)[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: loss = 0.6931291217994346 norm_grad = 0.6737700758918336 time=  5.489 stepnorm= 0.0003192478153122143\n",
      "Iteration 1: loss = 0.693129121799133 norm_grad = 0.6737483888890974 time=  5.538 stepnorm= 3.406545712480068e-05\n",
      "Iteration 2: loss = 0.6931291217987462 norm_grad = 0.6737483888887591 time=  4.12 stepnorm= 4.1456496462423e-05\n",
      "Iteration 3: loss = 0.6931291217987463 norm_grad = 0.6737483888880123 time=  4.609 stepnorm= 3.3610785339428466e-08\n",
      "Iteration 4: loss = 0.6931291217987463 norm_grad = 0.6737483888880119 time=  5.388 stepnorm= 7.035021928238492e-10\n",
      "Iteration 5: loss = 0.6931291217785687 norm_grad = 0.6737483888880119 time=  5.146 stepnorm= 0.0002833720895928164\n",
      "Iteration 6: loss = 0.6931291217381186 norm_grad = 0.6737483888035212 time=  5.363 stepnorm= 0.0003810740601944168\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_23100/3395820917.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     67\u001B[0m         \u001B[0mNS_loss\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     68\u001B[0m         \u001B[0mNS_x\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 69\u001B[1;33m         (w_NS,_timing, _loss, _samples)=ns.NS(w,loss_computation,gradient_computation,\n\u001B[0m\u001B[0;32m     70\u001B[0m                                                     Hv=hessian_vector_computation,hessian=hessian_computation, X=X, Y=Y, opt=opt, **loss_args)\n\u001B[0;32m     71\u001B[0m         \u001B[0mloss_collector\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mappend\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0m_loss\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mE:\\EECS 598\\cubic\\ns.py\u001B[0m in \u001B[0;36mNS\u001B[1;34m(w, loss, gradient, Hv, hessian, X, Y, opt, **kwargs)\u001B[0m\n\u001B[0;32m    202\u001B[0m         \u001B[1;31m# v = -torch.linalg.lstsq(torch.from_numpy(hessian), torch.from_numpy(grad)).solution.numpy()\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    203\u001B[0m         \u001B[1;31m# print(v.shape)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 204\u001B[1;33m         \u001B[0mmu\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mline_search\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mw\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mv\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mgrad\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mloss\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mX\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mY\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    205\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    206\u001B[0m         \u001B[0ms\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mmu\u001B[0m \u001B[1;33m*\u001B[0m \u001B[0mv\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mE:\\EECS 598\\cubic\\ns.py\u001B[0m in \u001B[0;36mline_search\u001B[1;34m(w, v, g, loss, X, Y, alpha, beta)\u001B[0m\n\u001B[0;32m     18\u001B[0m     \u001B[0ms\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;36m1\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     19\u001B[0m     \u001B[0mws\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mw\u001B[0m \u001B[1;33m+\u001B[0m \u001B[0ms\u001B[0m \u001B[1;33m*\u001B[0m \u001B[0mv\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 20\u001B[1;33m     \u001B[1;32mwhile\u001B[0m \u001B[0mloss\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mws\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mX\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mY\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m>\u001B[0m \u001B[0mloss_x\u001B[0m \u001B[1;33m+\u001B[0m \u001B[0malpha\u001B[0m \u001B[1;33m*\u001B[0m \u001B[0ms\u001B[0m \u001B[1;33m*\u001B[0m \u001B[0mdelta\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     21\u001B[0m         \u001B[0ms\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mbeta\u001B[0m \u001B[1;33m*\u001B[0m \u001B[0ms\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     22\u001B[0m         \u001B[0mws\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mw\u001B[0m \u001B[1;33m+\u001B[0m \u001B[0ms\u001B[0m \u001B[1;33m*\u001B[0m \u001B[0mv\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mE:\\EECS 598\\cubic\\loss_functions.py\u001B[0m in \u001B[0;36mlogistic_loss\u001B[1;34m(w, X, Y, alpha)\u001B[0m\n\u001B[0;32m     39\u001B[0m     \u001B[0md\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mX\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mshape\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     40\u001B[0m     \u001B[0mz\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mX\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdot\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mw\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 41\u001B[1;33m     \u001B[0ml\u001B[0m\u001B[1;33m=\u001B[0m \u001B[1;33m-\u001B[0m \u001B[1;33m(\u001B[0m\u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdot\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mlog_phi\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mz\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mY\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m+\u001B[0m\u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdot\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mones\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mn\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m-\u001B[0m\u001B[0mY\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mone_minus_log_phi\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mz\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m/\u001B[0m\u001B[0mn\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     42\u001B[0m     \u001B[0ml\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0ml\u001B[0m \u001B[1;33m+\u001B[0m \u001B[1;36m0.5\u001B[0m\u001B[1;33m*\u001B[0m  \u001B[0malpha\u001B[0m \u001B[1;33m*\u001B[0m \u001B[1;33m(\u001B[0m\u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mlinalg\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mnorm\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mw\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m**\u001B[0m \u001B[1;36m2\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     43\u001B[0m     \u001B[1;32mreturn\u001B[0m \u001B[0ml\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mE:\\EECS 598\\cubic\\loss_functions.py\u001B[0m in \u001B[0;36mone_minus_log_phi\u001B[1;34m(t)\u001B[0m\n\u001B[0;32m    282\u001B[0m     \u001B[0mout\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mempty\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mt\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msize\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdtype\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfloat\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    283\u001B[0m     \u001B[0mout\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0midx\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m=\u001B[0m \u001B[1;33m-\u001B[0m\u001B[0mt\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0midx\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m-\u001B[0m\u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mlog\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m+\u001B[0m\u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mexp\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m-\u001B[0m\u001B[0mt\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0midx\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 284\u001B[1;33m     \u001B[0mout\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;33m~\u001B[0m\u001B[0midx\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;33m-\u001B[0m\u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mlog\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m+\u001B[0m\u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mexp\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mt\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;33m~\u001B[0m\u001B[0midx\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    285\u001B[0m     \u001B[1;32mreturn\u001B[0m \u001B[0mout\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "n_runs = 1 # repeat experiments to lower effect of randomness\n",
    "\n",
    "# The following parameters are optional in the sense that default values are set if not specified.\n",
    "opt = {}\n",
    "\n",
    "### NS:\n",
    "opt['sketch_type'] = partial(less, lev_scores=True)\n",
    "opt['sketch_size'] = 12*d\n",
    "\n",
    "### TR and SCR:\n",
    "opt['penalty_increase_multiplier']=2.    # multiply by..\n",
    "opt['penalty_derease_multiplier']=2.     # divide by..\n",
    "opt['initial_penalty_parameter']=0.01\n",
    "opt['initial_tr_radius']=1\n",
    "opt['successful_treshold']=0.1\n",
    "opt['very_successful_treshold']=0.9\n",
    "\n",
    "opt['grad_tol']=1e-9\n",
    "opt['n_iterations'] = 20\n",
    "\n",
    "# Sampling\n",
    "opt['Hessian_sampling']=True\n",
    "opt['gradient_sampling']=False\n",
    "opt['initial_sample_size_Hessian']=0.025\n",
    "opt['initial_sample_size_gradient']=0.25\n",
    "opt['subproblem_solver']='lanczos'\n",
    "opt['unsuccessful_sample_scaling']=1.5\n",
    "opt['sample_scaling_Hessian']=1\n",
    "opt['sample_scaling_gradient']=1\n",
    "\n",
    "# Subproblem \n",
    "opt['subproblem_solver_SCR']='lanczos' # alternatives: lanczos, cauchy_point, exact\n",
    "opt['subproblem_solver_TR']='GLTR' # alternatives: GLTR, cauchy_point, exact, dog_leg, cg\n",
    "\n",
    "opt['solve_each_i-th_krylov_space']=1   \n",
    "opt['krylov_tol']=1e-1\n",
    "opt['exact_tol']=1e-2\n",
    "opt['keep_Q_matrix_in_memory']=True\n",
    "\n",
    "### SGD:\n",
    "opt['n_epochs_sgd']=10\n",
    "opt['learning_rate_sgd']=1e-1 # This SGD implementation expects a constant step size\n",
    "opt['batch_size_sgd']=0.001*n\n",
    "\n",
    "### SAGA:\n",
    "opt['n_epochs_saga']=10\n",
    "opt['learning_rate_saga']=1e-2\n",
    "\n",
    "\n",
    "\n",
    "######################\n",
    "#### Run Methods #####\n",
    "######################\n",
    "\n",
    "SCR=True\n",
    "TR= True\n",
    "SGD= True\n",
    "SAGA= True\n",
    "NS = True\n",
    "\n",
    "if NS:\n",
    "    loss_collector=[]\n",
    "    timings_collector=[]\n",
    "    samples_collector=[]\n",
    "\n",
    "    for k in range(n_runs):\n",
    "        NS_loss=[]\n",
    "        NS_x=[]\n",
    "        (w_NS,_timing, _loss, _samples)=ns.NS(w,loss_computation,gradient_computation,\n",
    "                                                    Hv=hessian_vector_computation,hessian=hessian_computation, X=X, Y=Y, opt=opt, **loss_args)\n",
    "        loss_collector.append(_loss)\n",
    "        timings_collector.append(_timing)\n",
    "        samples_collector.append(_samples)\n",
    "\n",
    "    NS_loss = [float(sum(col))/len(col) for col in zip(*loss_collector)]\n",
    "    NS_x = [float(sum(col))/len(col) for col in zip(*timings_collector)]\n",
    "    NS_samples= [float(sum(col))/len(col) for col in zip(*samples_collector)]\n",
    "\n",
    "\n",
    "if SCR:\n",
    "    loss_collector=[]\n",
    "    timings_collector=[]\n",
    "    samples_collector=[]\n",
    "    \n",
    "    for k in range(n_runs): \n",
    "        SCR_loss=[]\n",
    "        SCR_x=[]\n",
    "        (w_SCR,_timing, _loss, _samples)=scr.SCR(w,loss_computation,gradient_computation,\n",
    "                                                    Hv=hessian_vector_computation,hessian=hessian_computation, X=X, Y=Y, opt=opt, **loss_args)\n",
    "        loss_collector.append(_loss)\n",
    "        timings_collector.append(_timing)\n",
    "        samples_collector.append(_samples)\n",
    "\n",
    "    SCR_loss = [float(sum(col))/len(col) for col in zip(*loss_collector)]\n",
    "    SCR_x = [float(sum(col))/len(col) for col in zip(*timings_collector)]    \n",
    "    SCR_samples= [float(sum(col))/len(col) for col in zip(*samples_collector)] \n",
    "    \n",
    "if TR:\n",
    "    loss_collector=[]\n",
    "    timings_collector=[]\n",
    "    samples_collector=[]\n",
    "    \n",
    "    for k in range(n_runs): \n",
    "        TR_loss=[]\n",
    "        TR_x=[]\n",
    "        (w_TR,_timing, _loss, _samples)=tr.Trust_Region(w,loss_computation,gradient_computation,\n",
    "                                                    Hv=hessian_vector_computation, hessian=hessian_computation, X=X, Y=Y, opt=opt, **loss_args)\n",
    "        loss_collector.append(_loss)\n",
    "        timings_collector.append(_timing)\n",
    "        samples_collector.append(_samples)\n",
    "\n",
    "    TR_loss = [float(sum(col))/len(col) for col in zip(*loss_collector)]\n",
    "    TR_x = [float(sum(col))/len(col) for col in zip(*timings_collector)]    \n",
    "    TR_samples= [float(sum(col))/len(col) for col in zip(*samples_collector)] \n",
    "    \n",
    "if SGD:\n",
    "    loss_collector=[]\n",
    "    timings_collector=[]\n",
    "    samples_collector=[]\n",
    "    \n",
    "    for k in range(n_runs): \n",
    "        SGD_loss=[]\n",
    "        SGD_x=[]\n",
    "        (w_SGD,_timing, _loss, _samples)=sgd.SGD(w,loss_computation,gradient_computation, X=X, Y=Y, opt=opt, **loss_args)\n",
    "        loss_collector.append(_loss)\n",
    "        timings_collector.append(_timing)\n",
    "        samples_collector.append(_samples)\n",
    "\n",
    "    SGD_loss = [float(sum(col))/len(col) for col in zip(*loss_collector)]\n",
    "    SGD_x = [float(sum(col))/len(col) for col in zip(*timings_collector)]    \n",
    "    SGD_samples= [float(sum(col))/len(col) for col in zip(*samples_collector)] \n",
    "    \n",
    "if SAGA:\n",
    "    loss_collector=[]\n",
    "    timings_collector=[]\n",
    "    samples_collector=[]\n",
    "    \n",
    "    for k in range(n_runs): \n",
    "        SAGA_loss=[]\n",
    "        SAGA_x=[]\n",
    "        (w_SAGA,_timing, _loss, _samples)=saga.SAGA(w,loss_computation,gradient_computation, X=X, Y=Y, opt=opt, **loss_args)\n",
    "        loss_collector.append(_loss)\n",
    "        timings_collector.append(_timing)\n",
    "        samples_collector.append(_samples)\n",
    "\n",
    "    SAGA_loss = [float(sum(col))/len(col) for col in zip(*loss_collector)]\n",
    "    SAGA_x = [float(sum(col))/len(col) for col in zip(*timings_collector)]    \n",
    "    SAGA_samples= [float(sum(col))/len(col) for col in zip(*samples_collector)]     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "show_plots(x_limits_time=(0,2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pycharm-3c69b7d7",
   "language": "python",
   "display_name": "PyCharm (IOE 310)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}