{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn import linear_model, datasets\n",
    "import os\n",
    "import simplejson\n",
    "import sys\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_svmlight_file\n",
    "import dsdl #https://github.com/fkunstner/dataset-downloader.git\n",
    "\n",
    "import loss_functions, make_plots\n",
    "import scr, tr, sgd, saga, ns, sn\n",
    "from sketches import gaussian, srht, less, sparse_rademacher, rrs, sjlt, lvrg_sampling, sqrn_sampling\n",
    "\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) load data and initialize weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset w8a loaded\n",
      "n = 49749 d = 300\n"
     ]
    }
   ],
   "source": [
    "#####################\n",
    "#### Load  Data  ####\n",
    "#####################\n",
    "\n",
    "dataset_name='w8a'\n",
    "\n",
    "#logistic regression:\n",
    "if dataset_name=='a9a':\n",
    "    X, Y = load_svmlight_file('data/a9a')\n",
    "    X = X.toarray()\n",
    "    Y= [0 if e == -1 else e for e in Y]\n",
    "    Y=np.array(Y)      \n",
    "    d = X.shape[1]\n",
    "    n = X.shape[0]\n",
    "    w = np.zeros(d)\n",
    "\n",
    "if dataset_name=='w8a':\n",
    "    ds = dsdl.load(\"w8a\")\n",
    "    X, Y = ds.get_train()\n",
    "    X = X.toarray()\n",
    "    Y= [0 if e == -1 else e for e in Y]\n",
    "    Y=np.array(Y)      \n",
    "    d = X.shape[1]\n",
    "    n = X.shape[0]\n",
    "    w = np.zeros(d)\n",
    "    \n",
    "#multinominal (softmax) regression:\n",
    "elif dataset_name == 'mnist':\n",
    "    import scipy \n",
    "    X, Y = load_svmlight_file('data/mnist')\n",
    "    X = X.toarray()\n",
    "    nC = len(np.unique(Y))    \n",
    "    ## one-hot encoding of labels\n",
    "    data   = np.ones(len(Y))\n",
    "    indptr = np.arange(len(Y)+1)\n",
    "    ground_truth = scipy.sparse.csr_matrix((data, Y, indptr))\n",
    "    Y = ground_truth.todense() #gives a matrix with [Ground_Truth]i,j: is sample i in class j? (n x nC)\n",
    "    d = X.shape[1]\n",
    "    n = X.shape[0]\n",
    "    w = np.zeros(nC*d)\n",
    "\n",
    "#general function    \n",
    "elif dataset_name=='rosenbrock':\n",
    "    d=2\n",
    "    n=1 \n",
    "    w = np.zeros(d)\n",
    "\n",
    "print ('Dataset', dataset_name, 'loaded')    \n",
    "print ('n = ' + str(n) + ' d = ' + str(d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c) specify a loss, gradient, Hessian-vector-product and Hessian computations\n",
    "- (latter only needed for hard case)\n",
    "- functions need to have (w,X,Y,kwargs...) as input structure !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataset_name in {'a9a', 'w8a'}:\n",
    "    loss_computation = loss_functions.logistic_loss\n",
    "    gradient_computation = loss_functions.logistic_loss_gradient\n",
    "    hessian_vector_computation = loss_functions.logistic_loss_Hv\n",
    "    hessian_computation = loss_functions.logistic_loss_hessian\n",
    "    \n",
    "    # Additional arguments that are to be passed to the Loss, Gradient, etc. computations\n",
    "    loss_args= {}\n",
    "    loss_args['alpha'] = 1e-3 # regularizer\n",
    "\n",
    "if dataset_name in {'mnist'}:\n",
    "    loss_computation = loss_functions.softmax_loss\n",
    "    gradient_computation = loss_functions.softmax_loss_gradient\n",
    "    hessian_vector_computation = loss_functions.softmax_loss_Hv\n",
    "    hessian_computation = loss_functions.softmax_loss_hessian\n",
    "    \n",
    "    loss_args= {}\n",
    "    loss_args['alpha'] = 1e-3\n",
    "    loss_args['n_classes'] = nC # for multiclass (softmax) regression\n",
    "\n",
    "    \n",
    "elif dataset_name == 'rosenbrock':\n",
    "    loss_computation = loss_functions.rosenbrock_loss\n",
    "    gradient_computation = loss_functions.rosenbrock_gradient\n",
    "    hessian_vector_computation = loss_functions.rosenbrock_Hv\n",
    "    hessian_computation = loss_functions.rosenbrock_hessian\n",
    "    \n",
    "    loss_args= {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_plots(x_limits_time=None):\n",
    "    list_params=[]\n",
    "    list_loss=[]\n",
    "    list_x=[]\n",
    "    list_samples=[]\n",
    "    \n",
    "    log_scale=True\n",
    "    \n",
    "    over_time=True\n",
    "    over_iterations=True\n",
    "    over_epochs=True\n",
    "\n",
    "    if NS:\n",
    "        list_loss.append(NS_loss)\n",
    "        list_x.append(NS_x)\n",
    "        list_samples.append(NS_samples)\n",
    "        list_params.append('NS')\n",
    "\n",
    "    if SCR:\n",
    "        list_loss.append(SCR_loss)\n",
    "        list_x.append(SCR_x)\n",
    "        list_samples.append(SCR_samples)\n",
    "        list_params.append('SCR')\n",
    "\n",
    "    if SkCR:\n",
    "        list_loss.append(SkCR_loss)\n",
    "        list_x.append(SkCR_x)\n",
    "        list_samples.append(SkCR_samples)\n",
    "        list_params.append('SubG SkCR')\n",
    "\n",
    "    if TR:\n",
    "        list_loss.append(TR_loss)\n",
    "        list_x.append(TR_x)\n",
    "        list_samples.append(TR_samples)\n",
    "        list_params.append('TR')\n",
    "        \n",
    "    if SGD:\n",
    "        list_loss.append(SGD_loss)\n",
    "        list_x.append(SGD_x)\n",
    "        list_samples.append(SGD_samples)\n",
    "        list_params.append('SGD')\n",
    "    if SAGA:\n",
    "        list_loss.append(SAGA_loss)\n",
    "        list_x.append(SAGA_x)\n",
    "        list_samples.append(SAGA_samples)\n",
    "        list_params.append('SAGA')\n",
    "\n",
    "    if LVRG:\n",
    "        list_loss.append(LVRG_loss)\n",
    "        list_x.append(LVRG_x)\n",
    "        list_samples.append(LVRG_samples)\n",
    "        list_params.append('Lvrg SCR')\n",
    "\n",
    "    if SQRN:\n",
    "        list_loss.append(SQRN_loss)\n",
    "        list_x.append(SQRN_x)\n",
    "        list_samples.append(SQRN_samples)\n",
    "        list_params.append('SqRN SCR')\n",
    "\n",
    "    np.savez(f'time_{dataset_name}.npz', **{list_params[i]: list_x[i] for i in range(len(list_loss))})\n",
    "    np.savez(f'loss_{dataset_name}.npz', **{list_params[i]: list_loss[i] for i in range(len(list_loss))})\n",
    "    \n",
    "#     if over_time:\n",
    "#         make_plots.two_d_plot_time(list_loss,list_x,list_params,dataset_name, n, d, log_scale,x_limits=x_limits_time)\n",
    "        \n",
    "#     if over_iterations:\n",
    "#         make_plots.two_d_plot_iterations(list_loss,list_x,list_params,dataset_name, n, d, log_scale)\n",
    "        \n",
    "    # if over_epochs:\n",
    "    #     make_plots.two_d_plot_epochs(list_loss,list_samples,list_params,dataset_name, n, d, log_scale)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d) Set parameters and run methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Sketched Cubic Regularization ---\n",
      "\n",
      "- Subproblem_solver: exact\n",
      "- Hessian_sampling: True\n",
      "- Gradient_sampling: False\n",
      "- Sampling_scheme: adaptive \n",
      "\n",
      "Iteration 0: loss = 0.3523056592830344 norm_grad = 0.5625363177417217 time=  0.632 stepnorm= 1.6946433027176835\n",
      "Iteration 1: loss = 0.2545655008266055 norm_grad = 0.17323530328187872 time=  0.632 stepnorm= 1.4989687859798466\n",
      "Iteration 2: loss = 0.20831747591459363 norm_grad = 0.068329543754302 time=  0.605 stepnorm= 1.5620769434699406\n",
      "Iteration 3: loss = 0.18937723792211306 norm_grad = 0.026034745008332118 time=  0.566 stepnorm= 1.6116635702646602\n",
      "Iteration 4: loss = 0.1841663452020687 norm_grad = 0.009073997462624205 time=  0.559 stepnorm= 1.2838705198958145\n",
      "Iteration 5: loss = 0.18339806589315 norm_grad = 0.0026341240072191388 time=  0.549 stepnorm= 0.7169754386615558\n",
      "Iteration 6: loss = 0.18336747796947842 norm_grad = 0.0003886461877880125 time=  0.621 stepnorm= 0.18113437404772667\n",
      "Iteration 7: loss = 0.18336724133720705 norm_grad = 3.254179970186664e-05 time=  1.54 stepnorm= 0.016429889869097426\n",
      "Iteration 8: loss = 0.18336724102602553 norm_grad = 1.2235242589832487e-06 time=  1.584 stepnorm= 0.0005599619884392131\n",
      "Iteration 9: loss = 0.1833672410256595 norm_grad = 4.181300903919354e-08 time=  1.348 stepnorm= 1.9714930468451665e-05\n",
      "Iteration 10: loss = 0.18336724102565888 norm_grad = 1.800379239265149e-09 time=  1.67 stepnorm= 7.488693606229544e-07\n",
      "--- Sketched Cubic Regularization ---\n",
      "\n",
      "- Subproblem_solver: exact\n",
      "- Hessian_sampling: True\n",
      "- Gradient_sampling: False\n",
      "- Sampling_scheme: adaptive \n",
      "\n",
      "Iteration 0: loss = 0.3440735303193149 norm_grad = 0.5625363177417217 time=  0.566 stepnorm= 1.8382878078712628\n",
      "Iteration 1: loss = 0.2513771327810168 norm_grad = 0.1680279587460743 time=  0.614 stepnorm= 1.49165905519974\n",
      "Iteration 2: loss = 0.20821560021433605 norm_grad = 0.06614697492187573 time=  0.565 stepnorm= 1.4931960412228447\n",
      "Iteration 3: loss = 0.18952825188446362 norm_grad = 0.025827176958853587 time=  0.582 stepnorm= 1.5644262111721272\n",
      "Iteration 4: loss = 0.18410257528523863 norm_grad = 0.009262417505213326 time=  0.555 stepnorm= 1.3083996969526241\n",
      "Iteration 5: loss = 0.183388721881368 norm_grad = 0.0024777265641732823 time=  0.59 stepnorm= 0.6947999349086853\n",
      "Iteration 6: loss = 0.18336736516522262 norm_grad = 0.00029645116162494914 time=  0.651 stepnorm= 0.15546541446038117\n",
      "Iteration 7: loss = 0.18336724116416064 norm_grad = 2.188627531416565e-05 time=  1.263 stepnorm= 0.01211804575381972\n",
      "Iteration 8: loss = 0.18336724102604704 norm_grad = 7.689363063291687e-07 time=  1.186 stepnorm= 0.0003925345851493723\n",
      "Iteration 9: loss = 0.18336724102565938 norm_grad = 4.027296445717818e-08 time=  1.221 stepnorm= 2.0692100580126745e-05\n",
      "Iteration 10: loss = 0.18336724102565888 norm_grad = 1.4586387951625035e-09 time=  1.569 stepnorm= 6.859274138329206e-07\n",
      "--- Sketched Cubic Regularization ---\n",
      "\n",
      "- Subproblem_solver: exact\n",
      "- Hessian_sampling: True\n",
      "- Gradient_sampling: False\n",
      "- Sampling_scheme: adaptive \n",
      "\n",
      "Iteration 0: loss = 0.3506003060419806 norm_grad = 0.5625363177417217 time=  0.565 stepnorm= 1.7735603512583955\n",
      "Iteration 1: loss = 0.25504891332161433 norm_grad = 0.16706965008453645 time=  0.573 stepnorm= 1.4423589174574118\n",
      "Iteration 2: loss = 0.20898797045947232 norm_grad = 0.06755749884655496 time=  0.53 stepnorm= 1.519929562972598\n",
      "Iteration 3: loss = 0.18969701172820946 norm_grad = 0.026243238822887628 time=  0.533 stepnorm= 1.6038309790709846\n",
      "Iteration 4: loss = 0.1840826460169604 norm_grad = 0.009330134677682494 time=  0.56 stepnorm= 1.3321009681192784\n",
      "Iteration 5: loss = 0.1834209444604092 norm_grad = 0.0023514895768270557 time=  0.548 stepnorm= 0.7177953847111944\n",
      "Iteration 6: loss = 0.18336763402535514 norm_grad = 0.0005016969215604568 time=  0.653 stepnorm= 0.23521484056078168\n",
      "Iteration 7: loss = 0.1833672414303999 norm_grad = 4.108533830250292e-05 time=  1.262 stepnorm= 0.02103704251165864\n",
      "Iteration 8: loss = 0.1833672410262292 norm_grad = 1.3089455788279356e-06 time=  1.271 stepnorm= 0.0006715567669824567\n",
      "Iteration 9: loss = 0.1833672410256596 norm_grad = 5.122206017016332e-08 time=  1.274 stepnorm= 2.4980869632340973e-05\n",
      "Iteration 10: loss = 0.1833672410256589 norm_grad = 1.635220521166159e-09 time=  1.29 stepnorm= 8.759089023187357e-07\n",
      "--- Sketched Cubic Regularization ---\n",
      "\n",
      "- Subproblem_solver: exact\n",
      "- Hessian_sampling: True\n",
      "- Gradient_sampling: False\n",
      "- Sampling_scheme: adaptive \n",
      "\n",
      "Iteration 0: loss = 0.34459542022898926 norm_grad = 0.5625363177417217 time=  0.547 stepnorm= 1.7555604961363134\n",
      "Iteration 1: loss = 0.2509316121974867 norm_grad = 0.15920370442690163 time=  0.583 stepnorm= 1.4931192289737996\n",
      "Iteration 2: loss = 0.2068628046656651 norm_grad = 0.0637732302744277 time=  0.548 stepnorm= 1.5619836617896528\n",
      "Iteration 3: loss = 0.18902685004567815 norm_grad = 0.024360250078770167 time=  0.574 stepnorm= 1.5859678893920142\n",
      "Iteration 4: loss = 0.1839590306311828 norm_grad = 0.008605595036031203 time=  0.617 stepnorm= 1.3138281685223427\n",
      "Iteration 5: loss = 0.183386857803741 norm_grad = 0.0019713807748039497 time=  0.777 stepnorm= 0.6604226469607709\n",
      "Iteration 6: loss = 0.18336735514732377 norm_grad = 0.0002787227510530234 time=  0.781 stepnorm= 0.15086071165379727\n",
      "Iteration 7: loss = 0.1833672411489642 norm_grad = 2.2271963466686397e-05 time=  1.322 stepnorm= 0.011248794204272518\n",
      "Iteration 8: loss = 0.18336724102587926 norm_grad = 7.167284070542836e-07 time=  1.258 stepnorm= 0.00036886540117052607\n",
      "Iteration 9: loss = 0.1833672410256597 norm_grad = 2.7899784875844468e-08 time=  1.205 stepnorm= 1.7047022584187763e-05\n",
      "Iteration 10: loss = 0.18336724102565885 norm_grad = 1.7507628879823374e-09 time=  1.232 stepnorm= 9.935769800782597e-07\n",
      "--- Sketched Cubic Regularization ---\n",
      "\n",
      "- Subproblem_solver: exact\n",
      "- Hessian_sampling: True\n",
      "- Gradient_sampling: False\n",
      "- Sampling_scheme: adaptive \n",
      "\n",
      "Iteration 0: loss = 0.34747515160200504 norm_grad = 0.5625363177417217 time=  0.553 stepnorm= 1.7683482616944313\n",
      "Iteration 1: loss = 0.2545954532598852 norm_grad = 0.16444190925203941 time=  0.586 stepnorm= 1.4632456152840332\n",
      "Iteration 2: loss = 0.20890368874323115 norm_grad = 0.0673211714129573 time=  0.529 stepnorm= 1.5914280290506302\n",
      "Iteration 3: loss = 0.18942314619885428 norm_grad = 0.026584471977000382 time=  0.554 stepnorm= 1.6531911201142933\n",
      "Iteration 4: loss = 0.18409528994614213 norm_grad = 0.009486925998107597 time=  0.529 stepnorm= 1.31421180361898\n",
      "Iteration 5: loss = 0.1833903835936701 norm_grad = 0.0024512461263069343 time=  0.553 stepnorm= 0.7165160591544959\n",
      "Iteration 6: loss = 0.1833674268353743 norm_grad = 0.0003263692148931014 time=  0.642 stepnorm= 0.15680746257351522\n",
      "Iteration 7: loss = 0.1833672413032968 norm_grad = 2.8876993195400054e-05 time=  1.203 stepnorm= 0.014318788988521411\n",
      "Iteration 8: loss = 0.1833672410259854 norm_grad = 1.0705926688834796e-06 time=  1.19 stepnorm= 0.0005688328211399958\n",
      "Iteration 9: loss = 0.18336724102565916 norm_grad = 3.7603141323293475e-08 time=  1.148 stepnorm= 1.8949086541340925e-05\n",
      "Iteration 10: loss = 0.1833672410256589 norm_grad = 1.0552228621080765e-09 time=  1.465 stepnorm= 5.511538159754448e-07\n",
      "--- Sketched Cubic Regularization ---\n",
      "\n",
      "- Subproblem_solver: exact\n",
      "- Hessian_sampling: True\n",
      "- Gradient_sampling: False\n",
      "- Sampling_scheme: adaptive \n",
      "\n",
      "Iteration 0: loss = 0.3461662197539002 norm_grad = 0.5625363177417217 time=  0.347 stepnorm= 1.7883968702678372\n",
      "Iteration 1: loss = 0.2520122618302268 norm_grad = 0.16762081299074133 time=  0.31 stepnorm= 1.431874595958196\n",
      "Iteration 2: loss = 0.2077451939995503 norm_grad = 0.06714878297065234 time=  0.342 stepnorm= 1.5039168439026358\n",
      "Iteration 3: loss = 0.18905938987783108 norm_grad = 0.025908735731466886 time=  0.363 stepnorm= 1.5870901921883989\n",
      "Iteration 4: loss = 0.18395996023634573 norm_grad = 0.008882431147254373 time=  0.309 stepnorm= 1.275513761187315\n",
      "Iteration 5: loss = 0.18338239259905678 norm_grad = 0.002164633290936124 time=  0.365 stepnorm= 0.6431407447349531\n",
      "Iteration 6: loss = 0.18336730557880093 norm_grad = 0.000268816462674219 time=  0.459 stepnorm= 0.12934788232578948\n",
      "Iteration 7: loss = 0.18336724106930383 norm_grad = 1.70056061136611e-05 time=  1.019 stepnorm= 0.008437486143527418\n",
      "Iteration 8: loss = 0.1833672410256892 norm_grad = 4.3340198763107106e-07 time=  1.066 stepnorm= 0.000220518667287023\n",
      "Iteration 9: loss = 0.1833672410256589 norm_grad = 1.271935726958012e-08 time=  1.403 stepnorm= 5.568480294682632e-06\n",
      "--- Sketched Cubic Regularization ---\n",
      "\n",
      "- Subproblem_solver: exact\n",
      "- Hessian_sampling: True\n",
      "- Gradient_sampling: False\n",
      "- Sampling_scheme: adaptive \n",
      "\n",
      "Iteration 0: loss = 0.34286115432618486 norm_grad = 0.5625363177417217 time=  0.358 stepnorm= 1.7546668651112032\n",
      "Iteration 1: loss = 0.24907800760730686 norm_grad = 0.1616630381681741 time=  0.373 stepnorm= 1.4929788726522777\n",
      "Iteration 2: loss = 0.20560433112763501 norm_grad = 0.06363605438278055 time=  0.317 stepnorm= 1.5771734983528924\n",
      "Iteration 3: loss = 0.18841316587588514 norm_grad = 0.024487357274502537 time=  0.331 stepnorm= 1.5532239840072082\n",
      "Iteration 4: loss = 0.18387899942014552 norm_grad = 0.00822360595934524 time=  0.366 stepnorm= 1.247793090544593\n",
      "Iteration 5: loss = 0.1833787749429297 norm_grad = 0.002006167857565081 time=  0.318 stepnorm= 0.6066688830315713\n",
      "Iteration 6: loss = 0.18336728640740527 norm_grad = 0.0002164507038530839 time=  0.429 stepnorm= 0.11386916164772884\n",
      "Iteration 7: loss = 0.18336724106451827 norm_grad = 1.4380560193506003e-05 time=  1.103 stepnorm= 0.006927844613776114\n",
      "Iteration 8: loss = 0.18336724102569069 norm_grad = 4.094614527177381e-07 time=  1.032 stepnorm= 0.00020389779954855218\n",
      "Iteration 9: loss = 0.18336724102565893 norm_grad = 1.1778588104955123e-08 time=  1.012 stepnorm= 5.8772649530542515e-06\n",
      "--- Sketched Cubic Regularization ---\n",
      "\n",
      "- Subproblem_solver: exact\n",
      "- Hessian_sampling: True\n",
      "- Gradient_sampling: False\n",
      "- Sampling_scheme: adaptive \n",
      "\n",
      "Iteration 0: loss = 0.34349099155905094 norm_grad = 0.5625363177417217 time=  0.328 stepnorm= 1.7696623786011403\n",
      "Iteration 1: loss = 0.2504342514418434 norm_grad = 0.1639033958210869 time=  0.347 stepnorm= 1.4630206108764177\n",
      "Iteration 2: loss = 0.2075147532922964 norm_grad = 0.06556226639704678 time=  0.324 stepnorm= 1.5131601908035308\n",
      "Iteration 3: loss = 0.18935916446849888 norm_grad = 0.02590013292187819 time=  0.374 stepnorm= 1.5368411548459373\n",
      "Iteration 4: loss = 0.18412725125041254 norm_grad = 0.009222971963717179 time=  0.363 stepnorm= 1.2818109802332427\n",
      "Iteration 5: loss = 0.18339104092110362 norm_grad = 0.002719957253317659 time=  0.347 stepnorm= 0.6681205854458951\n",
      "Iteration 6: loss = 0.1833673627398638 norm_grad = 0.0003841667712627934 time=  0.463 stepnorm= 0.15086605318401086\n",
      "Iteration 7: loss = 0.1833672411352662 norm_grad = 2.2046342468967246e-05 time=  0.975 stepnorm= 0.011667032950791772\n",
      "Iteration 8: loss = 0.18336724102575638 norm_grad = 6.971004126594707e-07 time=  0.92 stepnorm= 0.00033805068444928865\n",
      "Iteration 9: loss = 0.18336724102565902 norm_grad = 2.1236968739248508e-08 time=  1.145 stepnorm= 1.0000238313028073e-05\n",
      "--- Sketched Cubic Regularization ---\n",
      "\n",
      "- Subproblem_solver: exact\n",
      "- Hessian_sampling: True\n",
      "- Gradient_sampling: False\n",
      "- Sampling_scheme: adaptive \n",
      "\n",
      "Iteration 0: loss = 0.3504662228904304 norm_grad = 0.5625363177417217 time=  0.371 stepnorm= 1.8047977651573748\n",
      "Iteration 1: loss = 0.2530367384490791 norm_grad = 0.17539588228173905 time=  0.333 stepnorm= 1.4578158996030581\n",
      "Iteration 2: loss = 0.2084217312871359 norm_grad = 0.06913117728032699 time=  0.336 stepnorm= 1.515729022131028\n",
      "Iteration 3: loss = 0.1895850731903345 norm_grad = 0.027443903107048317 time=  0.363 stepnorm= 1.5272185829132185\n",
      "Iteration 4: loss = 0.18407890561761467 norm_grad = 0.009797228558854962 time=  0.313 stepnorm= 1.2863290152553426\n",
      "Iteration 5: loss = 0.18339206083126675 norm_grad = 0.0024353262514393618 time=  0.321 stepnorm= 0.6706915518780442\n",
      "Iteration 6: loss = 0.18336735159273732 norm_grad = 0.0003578507982183862 time=  0.392 stepnorm= 0.16033132878798909\n",
      "Iteration 7: loss = 0.1833672411081689 norm_grad = 2.251860440581759e-05 time=  0.931 stepnorm= 0.011128794170516813\n",
      "Iteration 8: loss = 0.1833672410257127 norm_grad = 6.359406870238365e-07 time=  0.902 stepnorm= 0.0002941283381443001\n",
      "Iteration 9: loss = 0.183367241025659 norm_grad = 1.8368954581647052e-08 time=  0.924 stepnorm= 7.294825683515448e-06\n",
      "--- Sketched Cubic Regularization ---\n",
      "\n",
      "- Subproblem_solver: exact\n",
      "- Hessian_sampling: True\n",
      "- Gradient_sampling: False\n",
      "- Sampling_scheme: adaptive \n",
      "\n",
      "Iteration 0: loss = 0.34843010299654203 norm_grad = 0.5625363177417217 time=  0.298 stepnorm= 1.756204463123446\n",
      "Iteration 1: loss = 0.2536424940230147 norm_grad = 0.17009912300323393 time=  0.321 stepnorm= 1.4428871066291609\n",
      "Iteration 2: loss = 0.2084654633339418 norm_grad = 0.06808741192376966 time=  0.297 stepnorm= 1.5257873050771749\n",
      "Iteration 3: loss = 0.18948454470607323 norm_grad = 0.02670491576614431 time=  0.308 stepnorm= 1.6043445345669243\n",
      "Iteration 4: loss = 0.184056083594406 norm_grad = 0.009791550973491887 time=  0.314 stepnorm= 1.2992291380702579\n",
      "Iteration 5: loss = 0.18338630119412697 norm_grad = 0.0024545942740567023 time=  0.338 stepnorm= 0.6699771594471554\n",
      "Iteration 6: loss = 0.18336735741592836 norm_grad = 0.0003474768791818934 time=  0.334 stepnorm= 0.13615218188289085\n",
      "Iteration 7: loss = 0.18336724111418717 norm_grad = 2.374129326722352e-05 time=  0.94 stepnorm= 0.010980472148999781\n",
      "Iteration 8: loss = 0.18336724102572924 norm_grad = 6.163877940398803e-07 time=  0.906 stepnorm= 0.00031565926850980546\n",
      "Iteration 9: loss = 0.18336724102565902 norm_grad = 1.7705432265257404e-08 time=  0.917 stepnorm= 8.672340498544295e-06\n",
      "--- Sketched Cubic Regularization ---\n",
      "\n",
      "- Subproblem_solver: exact\n",
      "- Hessian_sampling: True\n",
      "- Gradient_sampling: False\n",
      "- Sampling_scheme: adaptive \n",
      "\n",
      "Iteration 0: loss = 0.34812357660294574 norm_grad = 0.5625363177417217 time=  0.315 stepnorm= 1.6842024016296535\n",
      "Iteration 1: loss = 0.25296126095429855 norm_grad = 0.1643596558398703 time=  0.281 stepnorm= 1.4472699123549662\n",
      "Iteration 2: loss = 0.20751750945018788 norm_grad = 0.06556555451515167 time=  0.306 stepnorm= 1.563844219599253\n",
      "Iteration 3: loss = 0.1890113728580625 norm_grad = 0.025165244633564413 time=  0.337 stepnorm= 1.5851437610008399\n",
      "Iteration 4: loss = 0.18402153329892984 norm_grad = 0.008753426508633819 time=  0.286 stepnorm= 1.2547912027434238\n",
      "Iteration 5: loss = 0.1833861829410059 norm_grad = 0.002298605330889858 time=  0.3 stepnorm= 0.6705749763558816\n",
      "Iteration 6: loss = 0.1833673616921025 norm_grad = 0.00031105617368724914 time=  0.374 stepnorm= 0.14057028857858866\n",
      "Iteration 7: loss = 0.18336724111194092 norm_grad = 2.319760409849074e-05 time=  0.85 stepnorm= 0.0115406059144721\n",
      "Iteration 8: loss = 0.18336724102570753 norm_grad = 6.024115668372665e-07 time=  0.883 stepnorm= 0.00031852943598229336\n",
      "Iteration 9: loss = 0.18336724102565896 norm_grad = 1.3984669882604946e-08 time=  0.943 stepnorm= 7.5700188563082465e-06\n",
      "--- Sketched Cubic Regularization ---\n",
      "\n",
      "- Subproblem_solver: exact\n",
      "- Hessian_sampling: True\n",
      "- Gradient_sampling: False\n",
      "- Sampling_scheme: adaptive \n",
      "\n",
      "Iteration 0: loss = 0.345258569082967 norm_grad = 0.5625363177417217 time=  0.324 stepnorm= 1.8054583029128235\n",
      "Iteration 1: loss = 0.25121942363137495 norm_grad = 0.17033817976900295 time=  0.373 stepnorm= 1.4320670669382243\n",
      "Iteration 2: loss = 0.20680744359461908 norm_grad = 0.06747465399993843 time=  0.391 stepnorm= 1.5491420209972724\n",
      "Iteration 3: loss = 0.18891559620882697 norm_grad = 0.026018779028955122 time=  0.333 stepnorm= 1.5654250139753407\n",
      "Iteration 4: loss = 0.18394332916724754 norm_grad = 0.009192174357978053 time=  0.376 stepnorm= 1.252707725813498\n",
      "Iteration 5: loss = 0.1833836534214396 norm_grad = 0.0021569176475846087 time=  0.351 stepnorm= 0.6228466188847375\n",
      "Iteration 6: loss = 0.1833673259858886 norm_grad = 0.00035237605585772643 time=  0.45 stepnorm= 0.12684817084927288\n",
      "Iteration 7: loss = 0.18336724107984176 norm_grad = 1.999410446623953e-05 time=  1.049 stepnorm= 0.009536659059125164\n",
      "Iteration 8: loss = 0.1833672410257005 norm_grad = 4.917053955445419e-07 time=  1.146 stepnorm= 0.00024335560576749615\n",
      "Iteration 9: loss = 0.18336724102565893 norm_grad = 1.4044521180384192e-08 time=  0.929 stepnorm= 6.511683537407369e-06\n",
      "--- Sketched Cubic Regularization ---\n",
      "\n",
      "- Subproblem_solver: exact\n",
      "- Hessian_sampling: True\n",
      "- Gradient_sampling: False\n",
      "- Sampling_scheme: adaptive \n",
      "\n",
      "Iteration 0: loss = 0.3461742641476137 norm_grad = 0.5625363177417217 time=  0.35 stepnorm= 1.693709776831512\n",
      "Iteration 1: loss = 0.25017228933089264 norm_grad = 0.16464933482669888 time=  0.351 stepnorm= 1.4443391221130972\n",
      "Iteration 2: loss = 0.20683969446666498 norm_grad = 0.062410933629955916 time=  0.306 stepnorm= 1.5636076856391379\n",
      "Iteration 3: loss = 0.1888552643873332 norm_grad = 0.024449093707656803 time=  0.301 stepnorm= 1.5828916364359475\n",
      "Iteration 4: loss = 0.18398392848020872 norm_grad = 0.008319359475825586 time=  0.333 stepnorm= 1.2825403805071283\n",
      "Iteration 5: loss = 0.18338614973199818 norm_grad = 0.0020522974381762564 time=  0.304 stepnorm= 0.6618473737465285\n",
      "Iteration 6: loss = 0.18336732288914623 norm_grad = 0.00029135688306446806 time=  0.33 stepnorm= 0.1417623581542591\n",
      "Iteration 7: loss = 0.1833672410825554 norm_grad = 1.8762031636819067e-05 time=  0.923 stepnorm= 0.009656567501983394\n",
      "Iteration 8: loss = 0.18336724102570215 norm_grad = 4.794249405580021e-07 time=  0.904 stepnorm= 0.0002600645708850601\n",
      "Iteration 9: loss = 0.18336724102565896 norm_grad = 1.2815446813445225e-08 time=  0.935 stepnorm= 7.173457352887171e-06\n",
      "--- Sketched Cubic Regularization ---\n",
      "\n",
      "- Subproblem_solver: exact\n",
      "- Hessian_sampling: True\n",
      "- Gradient_sampling: False\n",
      "- Sampling_scheme: adaptive \n",
      "\n",
      "Iteration 0: loss = 0.3407895559187859 norm_grad = 0.5625363177417217 time=  0.302 stepnorm= 1.787759906438825\n",
      "Iteration 1: loss = 0.2491385951607445 norm_grad = 0.16092236198656884 time=  0.302 stepnorm= 1.4421639266350859\n",
      "Iteration 2: loss = 0.20639920401772974 norm_grad = 0.06364678307195644 time=  0.324 stepnorm= 1.5489150983454691\n",
      "Iteration 3: loss = 0.18909588938383678 norm_grad = 0.024774210572195567 time=  0.361 stepnorm= 1.5381741587920534\n",
      "Iteration 4: loss = 0.1840315481694333 norm_grad = 0.00890995672536009 time=  0.295 stepnorm= 1.2727785189112448\n",
      "Iteration 5: loss = 0.18338346969132152 norm_grad = 0.0023292372666899866 time=  0.317 stepnorm= 0.6796668254545248\n",
      "Iteration 6: loss = 0.18336731318431912 norm_grad = 0.0002888417075201676 time=  0.417 stepnorm= 0.1326388000055962\n",
      "Iteration 7: loss = 0.1833672410751325 norm_grad = 1.7782764020016398e-05 time=  0.904 stepnorm= 0.008985409755846404\n",
      "Iteration 8: loss = 0.18336724102569124 norm_grad = 4.628332178449937e-07 time=  0.988 stepnorm= 0.00023617708006779068\n",
      "Iteration 9: loss = 0.18336724102565893 norm_grad = 1.1699366839077422e-08 time=  1.155 stepnorm= 6.0556402790384275e-06\n",
      "--- Sketched Cubic Regularization ---\n",
      "\n",
      "- Subproblem_solver: exact\n",
      "- Hessian_sampling: True\n",
      "- Gradient_sampling: False\n",
      "- Sampling_scheme: adaptive \n",
      "\n",
      "Iteration 0: loss = 0.3438338458744751 norm_grad = 0.5625363177417217 time=  0.356 stepnorm= 1.811576522497589\n",
      "Iteration 1: loss = 0.25049905440438247 norm_grad = 0.16739049099513087 time=  0.327 stepnorm= 1.4587887608501393\n",
      "Iteration 2: loss = 0.20705107430737016 norm_grad = 0.0660571846699023 time=  0.356 stepnorm= 1.5290612255639977\n",
      "Iteration 3: loss = 0.18906658990974334 norm_grad = 0.025524665234968578 time=  0.359 stepnorm= 1.5519466222044567\n",
      "Iteration 4: loss = 0.184053884459695 norm_grad = 0.008934100496722037 time=  0.296 stepnorm= 1.2435464799569298\n",
      "Iteration 5: loss = 0.18338467582355897 norm_grad = 0.002486950834544078 time=  0.316 stepnorm= 0.651657294691584\n",
      "Iteration 6: loss = 0.1833673314139433 norm_grad = 0.0003099187338140679 time=  0.381 stepnorm= 0.1331352542165654\n",
      "Iteration 7: loss = 0.18336724107810856 norm_grad = 2.2217942758525263e-05 time=  1.095 stepnorm= 0.009753116609372364\n",
      "Iteration 8: loss = 0.18336724102569854 norm_grad = 4.713233507715895e-07 time=  0.976 stepnorm= 0.00024623682083771805\n",
      "Iteration 9: loss = 0.1833672410256589 norm_grad = 1.2689171800407647e-08 time=  0.906 stepnorm= 6.759113285974378e-06\n",
      "--- Subsampled Cubic Regularization ---\n",
      "\n",
      "- Subproblem_solver: exact\n",
      "- Hessian_sampling: True\n",
      "- Gradient_sampling: False\n",
      "- Sampling_scheme: adaptive \n",
      "\n",
      "Iteration 0: loss = 0.3357269357738652 norm_grad = 0.5625363177417217 time=  0.063 penalty= 0.005 stepnorm= 1.9664167690978838 Samples Hessian= 1243 samples Gradient= 49749 \n",
      "\n",
      "Iteration 1: loss = 0.24626482765356816 norm_grad = 0.15284735905367958 time=  0.084 penalty= 0.0025 stepnorm= 1.5466866573078513 Samples Hessian= 1243 samples Gradient= 49749 \n",
      "\n",
      "Iteration 2: loss = 0.2048877693805716 norm_grad = 0.06095390848205609 time=  0.097 penalty= 0.00125 stepnorm= 1.5542786867591938 Samples Hessian= 2010 samples Gradient= 49749 \n",
      "\n",
      "Iteration 3: loss = 0.18823246978178174 norm_grad = 0.02354631077162488 time=  0.098 penalty= 0.000625 stepnorm= 1.5974095031162396 Samples Hessian= 1990 samples Gradient= 49749 \n",
      "\n",
      "Iteration 4: loss = 0.18395231790347605 norm_grad = 0.008219442632395798 time=  0.099 penalty= 0.0003125 stepnorm= 1.2166698124680368 Samples Hessian= 1884 samples Gradient= 49749 \n",
      "\n",
      "Iteration 5: loss = 0.1833896803228048 norm_grad = 0.00213552968626153 time=  0.092 penalty= 0.00015625 stepnorm= 0.6652923444445087 Samples Hessian= 3248 samples Gradient= 49749 \n",
      "\n",
      "Iteration 6: loss = 0.18336759936699332 norm_grad = 0.0003224205442195206 time=  0.128 penalty= 7.8125e-05 stepnorm= 0.15418283474594238 Samples Hessian= 10865 samples Gradient= 49749 \n",
      "\n",
      "Iteration 7: loss = 0.18336724102630192 norm_grad = 4.10055144680364e-05 time=  0.304 penalty= 3.90625e-05 stepnorm= 0.0195246230401032 Samples Hessian= 49749 samples Gradient= 49749 \n",
      "\n",
      "Iteration 8: loss = 0.18336724102565888 norm_grad = 6.527615955723651e-08 time=  0.36 penalty= 1.953125e-05 stepnorm= 2.5935158389577724e-05 Samples Hessian= 49749 samples Gradient= 49749 \n",
      "\n",
      "--- Subsampled Cubic Regularization ---\n",
      "\n",
      "- Subproblem_solver: exact\n",
      "- Hessian_sampling: True\n",
      "- Gradient_sampling: False\n",
      "- Sampling_scheme: adaptive \n",
      "\n",
      "Iteration 0: loss = 0.34514533453966956 norm_grad = 0.5625363177417217 time=  0.072 penalty= 0.005 stepnorm= 1.8372976846754587 Samples Hessian= 1243 samples Gradient= 49749 \n",
      "\n",
      "Iteration 1: loss = 0.24965212583744886 norm_grad = 0.1597758843856044 time=  0.086 penalty= 0.0025 stepnorm= 1.5360286376779024 Samples Hessian= 1243 samples Gradient= 49749 \n",
      "\n",
      "Iteration 2: loss = 0.20673484711050122 norm_grad = 0.062428753866523475 time=  0.096 penalty= 0.00125 stepnorm= 1.5665417747760275 Samples Hessian= 1779 samples Gradient= 49749 \n",
      "\n",
      "Iteration 3: loss = 0.1888601746743697 norm_grad = 0.02442956232667484 time=  0.093 penalty= 0.000625 stepnorm= 1.6011382239028522 Samples Hessian= 1710 samples Gradient= 49749 \n",
      "\n",
      "Iteration 4: loss = 0.18403782980333555 norm_grad = 0.008362991716307541 time=  0.094 penalty= 0.0003125 stepnorm= 1.2976389842024787 Samples Hessian= 1637 samples Gradient= 49749 \n",
      "\n",
      "Iteration 5: loss = 0.18339353941831724 norm_grad = 0.0022519261088326994 time=  0.087 penalty= 0.00015625 stepnorm= 0.7148535229104203 Samples Hessian= 2493 samples Gradient= 49749 \n",
      "\n",
      "Iteration 6: loss = 0.18336765119195408 norm_grad = 0.00041770775638544995 time=  0.114 penalty= 7.8125e-05 stepnorm= 0.16839072251669637 Samples Hessian= 8215 samples Gradient= 49749 \n",
      "\n",
      "Iteration 7: loss = 0.18336724102629337 norm_grad = 4.389164215308912e-05 time=  0.295 penalty= 3.90625e-05 stepnorm= 0.021316872131450254 Samples Hessian= 49749 samples Gradient= 49749 \n",
      "\n",
      "Iteration 8: loss = 0.1833672410256589 norm_grad = 7.740656297947101e-08 time=  0.298 penalty= 1.953125e-05 stepnorm= 2.515565111010963e-05 Samples Hessian= 49749 samples Gradient= 49749 \n",
      "\n",
      "--- Subsampled Cubic Regularization ---\n",
      "\n",
      "- Subproblem_solver: exact\n",
      "- Hessian_sampling: True\n",
      "- Gradient_sampling: False\n",
      "- Sampling_scheme: adaptive \n",
      "\n",
      "Iteration 0: loss = 0.3407354162081628 norm_grad = 0.5625363177417217 time=  0.063 penalty= 0.005 stepnorm= 1.9283477659393478 Samples Hessian= 1243 samples Gradient= 49749 \n",
      "\n",
      "Iteration 1: loss = 0.2471691552632928 norm_grad = 0.15372978701412507 time=  0.093 penalty= 0.0025 stepnorm= 1.517428260277362 Samples Hessian= 1243 samples Gradient= 49749 \n",
      "\n",
      "Iteration 2: loss = 0.2055974682099132 norm_grad = 0.056718090665532635 time=  0.09 penalty= 0.00125 stepnorm= 1.5818221489463764 Samples Hessian= 2008 samples Gradient= 49749 \n",
      "\n",
      "Iteration 3: loss = 0.18870659631869524 norm_grad = 0.022150093391308297 time=  0.09 penalty= 0.000625 stepnorm= 1.6317932992294457 Samples Hessian= 1848 samples Gradient= 49749 \n",
      "\n",
      "Iteration 4: loss = 0.18394361042498142 norm_grad = 0.008069611897033719 time=  0.086 penalty= 0.0003125 stepnorm= 1.3302271857836439 Samples Hessian= 1736 samples Gradient= 49749 \n",
      "\n",
      "Iteration 5: loss = 0.1833886189930015 norm_grad = 0.0019867529711462386 time=  0.096 penalty= 0.00015625 stepnorm= 0.673310847804563 Samples Hessian= 2613 samples Gradient= 49749 \n",
      "\n",
      "Iteration 6: loss = 0.18336750059762594 norm_grad = 0.0003335336898412574 time=  0.134 penalty= 7.8125e-05 stepnorm= 0.15245091788242313 Samples Hessian= 10201 samples Gradient= 49749 \n",
      "\n",
      "Iteration 7: loss = 0.18336724102599866 norm_grad = 3.909659481388066e-05 time=  0.305 penalty= 3.90625e-05 stepnorm= 0.016491721707213105 Samples Hessian= 49749 samples Gradient= 49749 \n",
      "\n",
      "Iteration 8: loss = 0.1833672410256589 norm_grad = 6.865897329541268e-08 time=  0.296 penalty= 1.953125e-05 stepnorm= 1.5572817274883246e-05 Samples Hessian= 49749 samples Gradient= 49749 \n",
      "\n",
      "--- Subsampled Cubic Regularization ---\n",
      "\n",
      "- Subproblem_solver: exact\n",
      "- Hessian_sampling: True\n",
      "- Gradient_sampling: False\n",
      "- Sampling_scheme: adaptive \n",
      "\n",
      "Iteration 0: loss = 0.34403746950457464 norm_grad = 0.5625363177417217 time=  0.061 penalty= 0.005 stepnorm= 1.8934527086898991 Samples Hessian= 1243 samples Gradient= 49749 \n",
      "\n",
      "Iteration 1: loss = 0.24977100269299027 norm_grad = 0.1639943559110064 time=  0.088 penalty= 0.0025 stepnorm= 1.516293437135766 Samples Hessian= 1243 samples Gradient= 49749 \n",
      "\n",
      "Iteration 2: loss = 0.20585188411035837 norm_grad = 0.06398265637333457 time=  0.087 penalty= 0.00125 stepnorm= 1.6113012152015778 Samples Hessian= 1939 samples Gradient= 49749 \n",
      "\n",
      "Iteration 3: loss = 0.1885235570859875 norm_grad = 0.024067512388684983 time=  0.088 penalty= 0.000625 stepnorm= 1.5899765540003163 Samples Hessian= 1717 samples Gradient= 49749 \n",
      "\n",
      "Iteration 4: loss = 0.1839939037192744 norm_grad = 0.008086274049314182 time=  0.074 penalty= 0.0003125 stepnorm= 1.3185143778849646 Samples Hessian= 1763 samples Gradient= 49749 \n",
      "\n",
      "Iteration 5: loss = 0.18341366774974727 norm_grad = 0.0023178588928555546 time=  0.088 penalty= 0.00015625 stepnorm= 0.6819936953285017 Samples Hessian= 2564 samples Gradient= 49749 \n",
      "\n",
      "Iteration 6: loss = 0.1833681307387432 norm_grad = 0.0006917689492863493 time=  0.115 penalty= 7.8125e-05 stepnorm= 0.18766384155236598 Samples Hessian= 9586 samples Gradient= 49749 \n",
      "\n",
      "Iteration 7: loss = 0.18336724102826082 norm_grad = 6.095161773371482e-05 time=  0.284 penalty= 3.90625e-05 stepnorm= 0.03153574361580416 Samples Hessian= 49749 samples Gradient= 49749 \n",
      "\n",
      "Iteration 8: loss = 0.1833672410256589 norm_grad = 1.5068873267328263e-07 time=  0.288 penalty= 1.953125e-05 stepnorm= 5.210422020245427e-05 Samples Hessian= 49749 samples Gradient= 49749 \n",
      "\n",
      "--- Subsampled Cubic Regularization ---\n",
      "\n",
      "- Subproblem_solver: exact\n",
      "- Hessian_sampling: True\n",
      "- Gradient_sampling: False\n",
      "- Sampling_scheme: adaptive \n",
      "\n",
      "Iteration 0: loss = 0.34794846217730796 norm_grad = 0.5625363177417217 time=  0.071 penalty= 0.005 stepnorm= 1.8088757842483592 Samples Hessian= 1243 samples Gradient= 49749 \n",
      "\n",
      "Iteration 1: loss = 0.24988774784653228 norm_grad = 0.16447856052993423 time=  0.097 penalty= 0.0025 stepnorm= 1.5601966497018582 Samples Hessian= 1243 samples Gradient= 49749 \n",
      "\n",
      "Iteration 2: loss = 0.20681231791012958 norm_grad = 0.06260711631679253 time=  0.088 penalty= 0.00125 stepnorm= 1.6139205650453081 Samples Hessian= 1671 samples Gradient= 49749 \n",
      "\n",
      "Iteration 3: loss = 0.18897563041231383 norm_grad = 0.024997139233959572 time=  0.081 penalty= 0.000625 stepnorm= 1.6535586875172383 Samples Hessian= 1562 samples Gradient= 49749 \n",
      "\n",
      "Iteration 4: loss = 0.1839518899417511 norm_grad = 0.008877114156916256 time=  0.079 penalty= 0.0003125 stepnorm= 1.3504682595282498 Samples Hessian= 1488 samples Gradient= 49749 \n",
      "\n",
      "Iteration 5: loss = 0.18338796192197454 norm_grad = 0.001978152940628251 time=  0.089 penalty= 0.00015625 stepnorm= 0.7239136031425111 Samples Hessian= 2231 samples Gradient= 49749 \n",
      "\n",
      "Iteration 6: loss = 0.18336760809044564 norm_grad = 0.0002821188964440188 time=  0.114 penalty= 7.8125e-05 stepnorm= 0.15738148965921864 Samples Hessian= 7765 samples Gradient= 49749 \n",
      "\n",
      "Iteration 7: loss = 0.1833672410263174 norm_grad = 4.559072550782132e-05 time=  0.287 penalty= 3.90625e-05 stepnorm= 0.01949404222525785 Samples Hessian= 49749 samples Gradient= 49749 \n",
      "\n",
      "Iteration 8: loss = 0.18336724102565893 norm_grad = 7.862631126431696e-08 time=  0.29 penalty= 1.953125e-05 stepnorm= 2.494836786955009e-05 Samples Hessian= 49749 samples Gradient= 49749 \n",
      "\n",
      "--- Trust Region ---\n",
      "\n",
      "- Subproblem_solver: GLTR\n",
      "- Hessian_sampling: True\n",
      "- Gradient_sampling: False\n",
      "- Sampling_scheme: exponential \n",
      "\n",
      "Iteration 0: loss = 0.38739639417251714 norm_grad = 0.20182487059833895 time=  0.050011 tr_radius= 1 stepnorm= 1.0017960287677041 Samples Hessian= 1246 samples Gradient= 49749 \n",
      "\n",
      "Iteration 1: loss = 0.2848585167773318 norm_grad = 0.08048415635889357 time=  0.050012 tr_radius= 1 stepnorm= 0.7970958970593264 Samples Hessian= 1247 samples Gradient= 49749 \n",
      "\n",
      "Iteration 2: loss = 0.23281152497156918 norm_grad = 0.03194706637139433 time=  0.050012 tr_radius= 2.0 stepnorm= 1.017751192448294 Samples Hessian= 1249 samples Gradient= 49749 \n",
      "\n",
      "Iteration 3: loss = 0.19479286445189567 norm_grad = 0.015464753009639141 time=  0.056012000000000006 tr_radius= 4.0 stepnorm= 4.007403588178787 Samples Hessian= 1253 samples Gradient= 49749 \n",
      "\n",
      "Iteration 4: loss = 0.18491323990911035 norm_grad = 0.0032085866500096527 time=  0.05100999999999997 tr_radius= 4.0 stepnorm= 2.0797410146235227 Samples Hessian= 1259 samples Gradient= 49749 \n",
      "\n",
      "Iteration 5: loss = 0.18379211367074832 norm_grad = 0.0025924112643516102 time=  0.05201300000000003 tr_radius= 8.0 stepnorm= 1.3233995042300382 Samples Hessian= 1270 samples Gradient= 49749 \n",
      "\n",
      "unscuccesful iteration\n",
      "Iteration 6: loss = 0.18379211367074832 norm_grad = 0.0025924112643516102 time=  0.03300799999999998 tr_radius= 8.0 stepnorm= 2.17017834917193 Samples Hessian= 1288 samples Gradient= 49749 \n",
      "\n",
      "unscuccesful iteration\n",
      "Iteration 7: loss = 0.18379211367074832 norm_grad = 0.0025924112643516102 time=  0.03500599999999998 tr_radius= 4.0 stepnorm= 2.201648897802532 Samples Hessian= 1319 samples Gradient= 49749 \n",
      "\n",
      "Iteration 8: loss = 0.18351164860057798 norm_grad = 0.0013282339993303082 time=  0.05701400000000001 tr_radius= 2.0 stepnorm= 0.6309690012903171 Samples Hessian= 1373 samples Gradient= 49749 \n",
      "\n",
      "Iteration 9: loss = 0.18338078714600942 norm_grad = 0.000323949537953192 time=  0.05401300000000003 tr_radius= 2.0 stepnorm= 0.30896756529314506 Samples Hessian= 1464 samples Gradient= 49749 \n",
      "\n",
      "Iteration 10: loss = 0.1833688000585501 norm_grad = 8.389346887065239e-05 time=  0.05601099999999998 tr_radius= 4.0 stepnorm= 0.11474998652781296 Samples Hessian= 1622 samples Gradient= 49749 \n",
      "\n",
      "Iteration 11: loss = 0.18336746601962917 norm_grad = 3.3929350609898075e-05 time=  0.05701400000000001 tr_radius= 4.0 stepnorm= 0.04813143124684033 Samples Hessian= 1892 samples Gradient= 49749 \n",
      "\n",
      "Iteration 12: loss = 0.18336731035493387 norm_grad = 1.7280161632122028e-05 time=  0.057011000000000034 tr_radius= 4.0 stepnorm= 0.02151825023835696 Samples Hessian= 2355 samples Gradient= 49749 \n",
      "\n",
      "Iteration 13: loss = 0.1833672443341118 norm_grad = 3.731406345849094e-06 time=  0.06501799999999991 tr_radius= 4.0 stepnorm= 0.009405096868276508 Samples Hessian= 3150 samples Gradient= 49749 \n",
      "\n",
      "Iteration 14: loss = 0.18336724122928624 norm_grad = 1.0152584277291625e-06 time=  0.08601600000000009 tr_radius= 4.0 stepnorm= 0.002126708169746914 Samples Hessian= 4513 samples Gradient= 49749 \n",
      "\n",
      "Iteration 15: loss = 0.18336724106609742 norm_grad = 4.0758291204396813e-07 time=  0.10602400000000001 tr_radius= 4.0 stepnorm= 0.0006052358230364843 Samples Hessian= 6850 samples Gradient= 49749 \n",
      "\n",
      "Iteration 16: loss = 0.18336724102803004 norm_grad = 9.861606927404555e-08 time=  0.12802899999999995 tr_radius= 4.0 stepnorm= 0.00024967686497073956 Samples Hessian= 10859 samples Gradient= 49749 \n",
      "\n",
      "Iteration 17: loss = 0.1833672410257115 norm_grad = 1.4670120299803307e-08 time=  0.17003900000000005 tr_radius= 4.0 stepnorm= 4.51691668244889e-05 Samples Hessian= 17734 samples Gradient= 49749 \n",
      "\n",
      "Iteration 18: loss = 0.18336724102565954 norm_grad = 1.6355604129495255e-09 time=  0.25105500000000003 tr_radius= 8.0 stepnorm= 7.301189788680582e-06 Samples Hessian= 29526 samples Gradient= 49749 \n",
      "\n",
      "--- Trust Region ---\n",
      "\n",
      "- Subproblem_solver: GLTR\n",
      "- Hessian_sampling: True\n",
      "- Gradient_sampling: False\n",
      "- Sampling_scheme: exponential \n",
      "\n",
      "Iteration 0: loss = 0.3855089507139079 norm_grad = 0.19816942394108517 time=  0.048011 tr_radius= 1 stepnorm= 1.0019283293390158 Samples Hessian= 1246 samples Gradient= 49749 \n",
      "\n",
      "Iteration 1: loss = 0.2850659941171621 norm_grad = 0.08021351270322286 time=  0.050012 tr_radius= 1 stepnorm= 0.7904592646410719 Samples Hessian= 1247 samples Gradient= 49749 \n",
      "\n",
      "Iteration 2: loss = 0.2331911316285046 norm_grad = 0.03212823122184761 time=  0.044010999999999995 tr_radius= 2.0 stepnorm= 1.0124584769968394 Samples Hessian= 1249 samples Gradient= 49749 \n",
      "\n",
      "Iteration 3: loss = 0.19565682843217802 norm_grad = 0.015392315618108198 time=  0.055012000000000005 tr_radius= 4.0 stepnorm= 4.021189570341844 Samples Hessian= 1253 samples Gradient= 49749 \n",
      "\n",
      "Iteration 4: loss = 0.18622079374571415 norm_grad = 0.004451375013447721 time=  0.045009999999999994 tr_radius= 4.0 stepnorm= 2.5830107397176025 Samples Hessian= 1259 samples Gradient= 49749 \n",
      "\n",
      "Iteration 5: loss = 0.18431357930182032 norm_grad = 0.003146850895992901 time=  0.055012000000000005 tr_radius= 4.0 stepnorm= 1.8352028399819194 Samples Hessian= 1270 samples Gradient= 49749 \n",
      "\n",
      "Iteration 6: loss = 0.18370879754393785 norm_grad = 0.0015190218395428677 time=  0.05201 tr_radius= 4.0 stepnorm= 1.1705083027077223 Samples Hessian= 1288 samples Gradient= 49749 \n",
      "\n",
      "Iteration 7: loss = 0.1835984445255191 norm_grad = 0.0016610140971629664 time=  0.051012 tr_radius= 4.0 stepnorm= 0.8152363119188966 Samples Hessian= 1319 samples Gradient= 49749 \n",
      "\n",
      "Iteration 8: loss = 0.18343727420299696 norm_grad = 0.000576431964996111 time=  0.056012000000000006 tr_radius= 4.0 stepnorm= 0.46949994541944995 Samples Hessian= 1373 samples Gradient= 49749 \n",
      "\n",
      "Iteration 9: loss = 0.1833760664004821 norm_grad = 0.0002508671424451084 time=  0.05401299999999998 tr_radius= 4.0 stepnorm= 0.303812579504344 Samples Hessian= 1464 samples Gradient= 49749 \n",
      "\n",
      "Iteration 10: loss = 0.18336825600485454 norm_grad = 7.050689769712367e-05 time=  0.05401100000000003 tr_radius= 4.0 stepnorm= 0.10835525667406536 Samples Hessian= 1622 samples Gradient= 49749 \n",
      "\n",
      "Iteration 11: loss = 0.18336736496212833 norm_grad = 2.4720945884661348e-05 time=  0.05901499999999993 tr_radius= 4.0 stepnorm= 0.03630629175774477 Samples Hessian= 1892 samples Gradient= 49749 \n",
      "\n",
      "Iteration 12: loss = 0.18336727300598463 norm_grad = 1.5221745205340654e-05 time=  0.05901400000000001 tr_radius= 4.0 stepnorm= 0.01463188888793843 Samples Hessian= 2355 samples Gradient= 49749 \n",
      "\n",
      "Iteration 13: loss = 0.18336725092331457 norm_grad = 8.487911826754708e-06 time=  0.0640130000000001 tr_radius= 4.0 stepnorm= 0.007107592333753936 Samples Hessian= 3150 samples Gradient= 49749 \n",
      "\n",
      "Iteration 14: loss = 0.18336724191144482 norm_grad = 2.2043285783122163e-06 time=  0.08301799999999993 tr_radius= 4.0 stepnorm= 0.0032924494350748887 Samples Hessian= 4513 samples Gradient= 49749 \n",
      "\n",
      "Iteration 15: loss = 0.1833672410824407 norm_grad = 6.763535758797009e-07 time=  0.10202200000000006 tr_radius= 4.0 stepnorm= 0.0010086388795423201 Samples Hessian= 6850 samples Gradient= 49749 \n",
      "\n",
      "Iteration 16: loss = 0.1833672410275358 norm_grad = 9.491999294204809e-08 time=  0.12802999999999987 tr_radius= 8.0 stepnorm= 0.00023393961583889923 Samples Hessian= 10859 samples Gradient= 49749 \n",
      "\n",
      "Iteration 17: loss = 0.18336724102567484 norm_grad = 8.99116185736002e-09 time=  0.17503900000000017 tr_radius= 16.0 stepnorm= 4.334574146294009e-05 Samples Hessian= 17734 samples Gradient= 49749 \n",
      "\n",
      "--- Trust Region ---\n",
      "\n",
      "- Subproblem_solver: GLTR\n",
      "- Hessian_sampling: True\n",
      "- Gradient_sampling: False\n",
      "- Sampling_scheme: exponential \n",
      "\n",
      "Iteration 0: loss = 0.38204559501318947 norm_grad = 0.17379482803061488 time=  0.050012 tr_radius= 1 stepnorm= 0.9590634767452721 Samples Hessian= 1246 samples Gradient= 49749 \n",
      "\n",
      "Iteration 1: loss = 0.2755218529688745 norm_grad = 0.06873791639654687 time=  0.04901 tr_radius= 2.0 stepnorm= 0.957463302135251 Samples Hessian= 1247 samples Gradient= 49749 \n",
      "\n",
      "Iteration 2: loss = 0.2269236891136813 norm_grad = 0.026721591975849824 time=  0.052013000000000004 tr_radius= 4.0 stepnorm= 1.1184172217892578 Samples Hessian= 1249 samples Gradient= 49749 \n",
      "\n",
      "Iteration 3: loss = 0.1970616629988461 norm_grad = 0.0156268551763047 time=  0.06001200000000001 tr_radius= 8.0 stepnorm= 5.296129555078329 Samples Hessian= 1253 samples Gradient= 49749 \n",
      "\n",
      "Iteration 4: loss = 0.188292928861361 norm_grad = 0.0055799694220444885 time=  0.05801300000000001 tr_radius= 8.0 stepnorm= 3.5169777070544357 Samples Hessian= 1259 samples Gradient= 49749 \n",
      "\n",
      "unscuccesful iteration\n",
      "Iteration 5: loss = 0.188292928861361 norm_grad = 0.0055799694220444885 time=  0.038007999999999986 tr_radius= 8.0 stepnorm= 3.1329006336035965 Samples Hessian= 1270 samples Gradient= 49749 \n",
      "\n",
      "Iteration 6: loss = 0.1859036472259838 norm_grad = 0.005373099002616004 time=  0.06601600000000002 tr_radius= 4.0 stepnorm= 2.7398427070497626 Samples Hessian= 1288 samples Gradient= 49749 \n",
      "\n",
      "Iteration 7: loss = 0.18435983183545723 norm_grad = 0.002358139114164655 time=  0.06101399999999996 tr_radius= 4.0 stepnorm= 1.8047451387669997 Samples Hessian= 1319 samples Gradient= 49749 \n",
      "\n",
      "Iteration 8: loss = 0.1837635830713598 norm_grad = 0.0018001771812881442 time=  0.059011000000000036 tr_radius= 4.0 stepnorm= 1.384000684492102 Samples Hessian= 1373 samples Gradient= 49749 \n",
      "\n",
      "Iteration 9: loss = 0.18350851898470305 norm_grad = 0.0008766602619412398 time=  0.06101499999999993 tr_radius= 4.0 stepnorm= 0.8091761866204875 Samples Hessian= 1464 samples Gradient= 49749 \n",
      "\n",
      "Iteration 10: loss = 0.1833925361329215 norm_grad = 0.0004422940391648773 time=  0.05902800000000008 tr_radius= 4.0 stepnorm= 0.4435004570644462 Samples Hessian= 1622 samples Gradient= 49749 \n",
      "\n",
      "Iteration 11: loss = 0.18337121700923922 norm_grad = 0.00018703588672533964 time=  0.058997999999999995 tr_radius= 4.0 stepnorm= 0.17299615564512508 Samples Hessian= 1892 samples Gradient= 49749 \n",
      "\n",
      "Iteration 12: loss = 0.18336803055690057 norm_grad = 9.329420515939074e-05 time=  0.06401499999999993 tr_radius= 4.0 stepnorm= 0.06986837834915674 Samples Hessian= 2355 samples Gradient= 49749 \n",
      "\n",
      "Iteration 13: loss = 0.1833676562414392 norm_grad = 8.093873131053495e-05 time=  0.07301600000000008 tr_radius= 4.0 stepnorm= 0.032002318920728336 Samples Hessian= 3150 samples Gradient= 49749 \n",
      "\n",
      "Iteration 14: loss = 0.1833672870422796 norm_grad = 2.4080894255748976e-05 time=  0.07801799999999992 tr_radius= 4.0 stepnorm= 0.01848929275427669 Samples Hessian= 4513 samples Gradient= 49749 \n",
      "\n",
      "Iteration 15: loss = 0.18336725149870997 norm_grad = 1.3514097153286297e-05 time=  0.09201999999999999 tr_radius= 4.0 stepnorm= 0.006446954759111897 Samples Hessian= 6850 samples Gradient= 49749 \n",
      "\n",
      "Iteration 16: loss = 0.18336724129766074 norm_grad = 1.2028500448880478e-06 time=  0.11402600000000007 tr_radius= 4.0 stepnorm= 0.0020605601791936776 Samples Hessian= 10859 samples Gradient= 49749 \n",
      "\n",
      "Iteration 17: loss = 0.18336724102935245 norm_grad = 1.6020650597147781e-07 time=  0.16703800000000002 tr_radius= 8.0 stepnorm= 0.0005168217001827389 Samples Hessian= 17734 samples Gradient= 49749 \n",
      "\n",
      "Iteration 18: loss = 0.18336724102569626 norm_grad = 1.4584162142141731e-08 time=  0.24705599999999994 tr_radius= 16.0 stepnorm= 5.998183233713514e-05 Samples Hessian= 29526 samples Gradient= 49749 \n",
      "\n",
      "--- Trust Region ---\n",
      "\n",
      "- Subproblem_solver: GLTR\n",
      "- Hessian_sampling: True\n",
      "- Gradient_sampling: False\n",
      "- Sampling_scheme: exponential \n",
      "\n",
      "Iteration 0: loss = 0.38674561086930953 norm_grad = 0.19929167413832058 time=  0.051012 tr_radius= 1 stepnorm= 1.0017521141993304 Samples Hessian= 1246 samples Gradient= 49749 \n",
      "\n",
      "Iteration 1: loss = 0.28503256262015336 norm_grad = 0.07968541650909824 time=  0.05601199999999999 tr_radius= 1 stepnorm= 0.800220718908506 Samples Hessian= 1247 samples Gradient= 49749 \n",
      "\n",
      "Iteration 2: loss = 0.22258990231445921 norm_grad = 0.029995173590613385 time=  0.05401200000000002 tr_radius= 2.0 stepnorm= 1.4826413218833345 Samples Hessian= 1249 samples Gradient= 49749 \n",
      "\n",
      "Iteration 3: loss = 0.19242432695802314 norm_grad = 0.013768929941028614 time=  0.06401399999999999 tr_radius= 4.0 stepnorm= 3.659195950646332 Samples Hessian= 1253 samples Gradient= 49749 \n",
      "\n",
      "unscuccesful iteration\n",
      "Iteration 4: loss = 0.19242432695802314 norm_grad = 0.013768929941028614 time=  0.04001000000000002 tr_radius= 8.0 stepnorm= 4.132524567040261 Samples Hessian= 1259 samples Gradient= 49749 \n",
      "\n",
      "Iteration 5: loss = 0.18476964841949872 norm_grad = 0.002926888097697035 time=  0.05501099999999998 tr_radius= 4.0 stepnorm= 1.9832432006822946 Samples Hessian= 1270 samples Gradient= 49749 \n",
      "\n",
      "Iteration 6: loss = 0.1837485674951536 norm_grad = 0.002448987696835664 time=  0.061014999999999986 tr_radius= 8.0 stepnorm= 1.2500254098542471 Samples Hessian= 1288 samples Gradient= 49749 \n",
      "\n",
      "unscuccesful iteration\n",
      "Iteration 7: loss = 0.1837485674951536 norm_grad = 0.002448987696835664 time=  0.03300900000000001 tr_radius= 8.0 stepnorm= 0.7343450679837563 Samples Hessian= 1319 samples Gradient= 49749 \n",
      "\n",
      "unscuccesful iteration\n",
      "Iteration 8: loss = 0.1837485674951536 norm_grad = 0.002448987696835664 time=  0.03700599999999998 tr_radius= 4.0 stepnorm= 0.9077659420230502 Samples Hessian= 1373 samples Gradient= 49749 \n",
      "\n",
      "Iteration 9: loss = 0.1834549547583283 norm_grad = 0.001005427678446784 time=  0.05801500000000004 tr_radius= 2.0 stepnorm= 0.48647618044861274 Samples Hessian= 1464 samples Gradient= 49749 \n",
      "\n",
      "Iteration 10: loss = 0.18338274480116742 norm_grad = 0.0003952335226483144 time=  0.05201 tr_radius= 4.0 stepnorm= 0.27324054728302094 Samples Hessian= 1622 samples Gradient= 49749 \n",
      "\n",
      "Iteration 11: loss = 0.18337066375040487 norm_grad = 0.00016207983672780402 time=  0.05501199999999995 tr_radius= 4.0 stepnorm= 0.13366358216090954 Samples Hessian= 1892 samples Gradient= 49749 \n",
      "\n",
      "Iteration 12: loss = 0.18336776356233597 norm_grad = 6.027894208431869e-05 time=  0.062014000000000014 tr_radius= 4.0 stepnorm= 0.0663101984238128 Samples Hessian= 2355 samples Gradient= 49749 \n",
      "\n",
      "Iteration 13: loss = 0.18336729429676996 norm_grad = 1.6774625420088886e-05 time=  0.06501600000000007 tr_radius= 4.0 stepnorm= 0.0244083622946141 Samples Hessian= 3150 samples Gradient= 49749 \n",
      "\n",
      "Iteration 14: loss = 0.1833672447451755 norm_grad = 4.275400998694989e-06 time=  0.0820169999999999 tr_radius= 4.0 stepnorm= 0.008011788851006384 Samples Hessian= 4513 samples Gradient= 49749 \n",
      "\n",
      "Iteration 15: loss = 0.18336724117017414 norm_grad = 7.909549629370857e-07 time=  0.101024 tr_radius= 4.0 stepnorm= 0.0019840741578926943 Samples Hessian= 6850 samples Gradient= 49749 \n",
      "\n",
      "Iteration 16: loss = 0.18336724102804575 norm_grad = 1.1008519001157663e-07 time=  0.1290300000000001 tr_radius= 8.0 stepnorm= 0.00040688054845377755 Samples Hessian= 10859 samples Gradient= 49749 \n",
      "\n",
      "Iteration 17: loss = 0.18336724102568625 norm_grad = 1.2462873197666027e-08 time=  0.18003799999999992 tr_radius= 16.0 stepnorm= 4.990989758012221e-05 Samples Hessian= 17734 samples Gradient= 49749 \n",
      "\n",
      "--- Trust Region ---\n",
      "\n",
      "- Subproblem_solver: GLTR\n",
      "- Hessian_sampling: True\n",
      "- Gradient_sampling: False\n",
      "- Sampling_scheme: exponential \n",
      "\n",
      "Iteration 0: loss = 0.3827726489159781 norm_grad = 0.19454616767476965 time=  0.051012 tr_radius= 1 stepnorm= 1.002728088697092 Samples Hessian= 1246 samples Gradient= 49749 \n",
      "\n",
      "Iteration 1: loss = 0.28594031482434795 norm_grad = 0.08132802838851078 time=  0.051011 tr_radius= 1 stepnorm= 0.7633397219390999 Samples Hessian= 1247 samples Gradient= 49749 \n",
      "\n",
      "Iteration 2: loss = 0.2336346892689674 norm_grad = 0.032527168670615 time=  0.053013000000000005 tr_radius= 2.0 stepnorm= 1.0080606623617356 Samples Hessian= 1249 samples Gradient= 49749 \n",
      "\n",
      "Iteration 3: loss = 0.19406244749177184 norm_grad = 0.013960409203767801 time=  0.054011 tr_radius= 4.0 stepnorm= 4.0025639011821035 Samples Hessian= 1253 samples Gradient= 49749 \n",
      "\n",
      "Iteration 4: loss = 0.18797071867158596 norm_grad = 0.004994498939938282 time=  0.051011999999999974 tr_radius= 4.0 stepnorm= 2.696883917974031 Samples Hessian= 1259 samples Gradient= 49749 \n",
      "\n",
      "unscuccesful iteration\n",
      "Iteration 5: loss = 0.18797071867158596 norm_grad = 0.004994498939938282 time=  0.030008000000000035 tr_radius= 4.0 stepnorm= 3.6825633801286624 Samples Hessian= 1270 samples Gradient= 49749 \n",
      "\n",
      "Iteration 6: loss = 0.18598345025282265 norm_grad = 0.006923435499218125 time=  0.052012999999999976 tr_radius= 2.0 stepnorm= 2.0001125121793146 Samples Hessian= 1288 samples Gradient= 49749 \n",
      "\n",
      "Iteration 7: loss = 0.1839367646076918 norm_grad = 0.00251556369113939 time=  0.05400900000000003 tr_radius= 2.0 stepnorm= 1.1860843217965995 Samples Hessian= 1319 samples Gradient= 49749 \n",
      "\n",
      "unscuccesful iteration\n",
      "Iteration 8: loss = 0.1839367646076918 norm_grad = 0.00251556369113939 time=  0.035007999999999984 tr_radius= 2.0 stepnorm= 1.2095662335510993 Samples Hessian= 1373 samples Gradient= 49749 \n",
      "\n",
      "Iteration 9: loss = 0.18343750788089475 norm_grad = 0.0006394926182178524 time=  0.052012 tr_radius= 1.0 stepnorm= 0.7293233222871444 Samples Hessian= 1464 samples Gradient= 49749 \n",
      "\n",
      "Iteration 10: loss = 0.1833824041485631 norm_grad = 0.0002971291448152868 time=  0.05901400000000001 tr_radius= 1.0 stepnorm= 0.3422827311791592 Samples Hessian= 1622 samples Gradient= 49749 \n",
      "\n",
      "Iteration 11: loss = 0.1833712632033022 norm_grad = 0.00013829303901717529 time=  0.05501100000000003 tr_radius= 1.0 stepnorm= 0.14519363450875675 Samples Hessian= 1892 samples Gradient= 49749 \n",
      "\n",
      "Iteration 12: loss = 0.18336889778016374 norm_grad = 0.0001631383020389195 time=  0.06401499999999993 tr_radius= 1.0 stepnorm= 0.05422432221386331 Samples Hessian= 2355 samples Gradient= 49749 \n",
      "\n",
      "Iteration 13: loss = 0.18336773479052565 norm_grad = 9.400779677386872e-05 time=  0.06401400000000002 tr_radius= 1.0 stepnorm= 0.03305638072263733 Samples Hessian= 3150 samples Gradient= 49749 \n",
      "\n",
      "Iteration 14: loss = 0.18336726687875568 norm_grad = 1.0198038574695158e-05 time=  0.07701999999999998 tr_radius= 1.0 stepnorm= 0.013825906746023058 Samples Hessian= 4513 samples Gradient= 49749 \n",
      "\n",
      "Iteration 15: loss = 0.1833672415529627 norm_grad = 2.0625963768360943e-06 time=  0.10002200000000006 tr_radius= 2.0 stepnorm= 0.005936132729200858 Samples Hessian= 6850 samples Gradient= 49749 \n",
      "\n",
      "Iteration 16: loss = 0.1833672410344809 norm_grad = 1.9538650032906278e-07 time=  0.12202599999999986 tr_radius= 4.0 stepnorm= 0.0006893797031240293 Samples Hessian= 10859 samples Gradient= 49749 \n",
      "\n",
      "Iteration 17: loss = 0.18336724102577015 norm_grad = 2.563085111788803e-08 time=  0.1770400000000001 tr_radius= 8.0 stepnorm= 9.889035336407126e-05 Samples Hessian= 17734 samples Gradient= 49749 \n",
      "\n",
      "Iteration 18: loss = 0.18336724102565946 norm_grad = 1.79357122678551e-09 time=  0.2510570000000001 tr_radius= 16.0 stepnorm= 1.0383756033482464e-05 Samples Hessian= 29526 samples Gradient= 49749 \n",
      "\n",
      "--- SGD ---\n",
      "Epoch 0: loss = 0.6931471805599445 norm_grad = 0.5367579726464249 time= 0.0\n",
      "Epoch 1: loss = 0.20361054858905991 norm_grad = 0.04002603457389341 time= 0.199\n",
      "Epoch 2: loss = 0.1934038129331979 norm_grad = 0.03529693741187921 time= 0.187\n",
      "Epoch 3: loss = 0.18905757285111197 norm_grad = 0.08107863571213526 time= 0.192\n",
      "Epoch 4: loss = 0.18686316565036856 norm_grad = 0.05080401453255014 time= 0.19\n",
      "Epoch 5: loss = 0.18567064018665302 norm_grad = 0.10906513131087009 time= 0.192\n",
      "Epoch 6: loss = 0.18500117628157772 norm_grad = 0.05608928561631324 time= 0.189\n",
      "Epoch 7: loss = 0.18455699754071817 norm_grad = 0.029958899916063896 time= 0.189\n",
      "Epoch 8: loss = 0.1842243179495257 norm_grad = 0.04245461187067359 time= 0.188\n",
      "Epoch 9: loss = 0.1840324953321994 norm_grad = 0.04775711654493907 time= 0.188\n",
      "--- SGD ---\n",
      "Epoch 0: loss = 0.6931471805599445 norm_grad = 0.6040264729366834 time= 0.001\n",
      "Epoch 1: loss = 0.20403287778599705 norm_grad = 0.044173400046592516 time= 0.179\n",
      "Epoch 2: loss = 0.19319790252090832 norm_grad = 0.0863430039968404 time= 0.185\n",
      "Epoch 3: loss = 0.1888907206710694 norm_grad = 0.07833971030935731 time= 0.192\n",
      "Epoch 4: loss = 0.18690662147012044 norm_grad = 0.05956356429856661 time= 0.182\n",
      "Epoch 5: loss = 0.1857538662512231 norm_grad = 0.026276145270762857 time= 0.178\n",
      "Epoch 6: loss = 0.18498274418689134 norm_grad = 0.026696680263305416 time= 0.178\n",
      "Epoch 7: loss = 0.18451730159044458 norm_grad = 0.023612854005516635 time= 0.178\n",
      "Epoch 8: loss = 0.18421713606297638 norm_grad = 0.07303693393320002 time= 0.187\n",
      "Epoch 9: loss = 0.1841080690174824 norm_grad = 0.07331193689989765 time= 0.19\n",
      "--- SGD ---\n",
      "Epoch 0: loss = 0.6931471805599445 norm_grad = 0.5567539945185199 time= 0.0\n",
      "Epoch 1: loss = 0.20353727954802772 norm_grad = 0.06559036774603726 time= 0.186\n",
      "Epoch 2: loss = 0.19312551610686243 norm_grad = 0.046017159008772894 time= 0.184\n",
      "Epoch 3: loss = 0.18911472056283102 norm_grad = 0.03703771948839262 time= 0.184\n",
      "Epoch 4: loss = 0.18700175820787426 norm_grad = 0.09747220242075587 time= 0.192\n",
      "Epoch 5: loss = 0.18581186993879018 norm_grad = 0.1492887573358857 time= 0.191\n",
      "Epoch 6: loss = 0.18508652185109273 norm_grad = 0.06077731857545207 time= 0.2\n",
      "Epoch 7: loss = 0.184603906380727 norm_grad = 0.0270129348897168 time= 0.19\n",
      "Epoch 8: loss = 0.18426394805955093 norm_grad = 0.036933301753678774 time= 0.195\n",
      "Epoch 9: loss = 0.18418857757724993 norm_grad = 0.03934034722057082 time= 0.184\n",
      "--- SGD ---\n",
      "Epoch 0: loss = 0.6931471805599445 norm_grad = 0.5810057487953779 time= 0.0\n",
      "Epoch 1: loss = 0.20314291198440174 norm_grad = 0.03679451722257398 time= 0.19\n",
      "Epoch 2: loss = 0.19299966134423363 norm_grad = 0.04501820126060695 time= 0.19\n",
      "Epoch 3: loss = 0.1888362573845096 norm_grad = 0.03422396925377667 time= 0.191\n",
      "Epoch 4: loss = 0.18686874746090876 norm_grad = 0.04755976243503743 time= 0.185\n",
      "Epoch 5: loss = 0.18571596371308258 norm_grad = 0.055810921828143976 time= 0.188\n",
      "Epoch 6: loss = 0.184983218328107 norm_grad = 0.027746314225284455 time= 0.192\n",
      "Epoch 7: loss = 0.18453047641645237 norm_grad = 0.05196923738567033 time= 0.183\n",
      "Epoch 8: loss = 0.1842040424649342 norm_grad = 0.0240268738744416 time= 0.182\n",
      "Epoch 9: loss = 0.18398142942519363 norm_grad = 0.026788807218392996 time= 0.187\n",
      "--- SGD ---\n",
      "Epoch 0: loss = 0.6931471805599445 norm_grad = 0.46200944582333753 time= 0.0\n",
      "Epoch 1: loss = 0.20351476027363535 norm_grad = 0.10839247931638651 time= 0.178\n",
      "Epoch 2: loss = 0.19328424750997705 norm_grad = 0.07227456503019959 time= 0.192\n",
      "Epoch 3: loss = 0.18902356375446688 norm_grad = 0.036920928799099535 time= 0.189\n",
      "Epoch 4: loss = 0.18710546843008874 norm_grad = 0.04933625034800345 time= 0.191\n",
      "Epoch 5: loss = 0.18588152732552074 norm_grad = 0.02901129159624104 time= 0.19\n",
      "Epoch 6: loss = 0.18505105273322467 norm_grad = 0.0777997282485274 time= 0.188\n",
      "Epoch 7: loss = 0.18459858013443892 norm_grad = 0.03128775086931779 time= 0.192\n",
      "Epoch 8: loss = 0.18422177268918316 norm_grad = 0.11422605047442454 time= 0.189\n",
      "Epoch 9: loss = 0.18403581121891557 norm_grad = 0.02445155598263331 time= 0.184\n"
     ]
    }
   ],
   "source": [
    "n_runs = 5 # repeat experiments to lower effect of randomness\n",
    "\n",
    "# The following parameters are optional in the sense that default values are set if not specified.\n",
    "opt = {}\n",
    "\n",
    "### NS:\n",
    "opt['sketch_type'] = lvrg_sampling\n",
    "opt['sketch_type_SkCR'] = sparse_rademacher\n",
    "opt['sketch_size'] = 100*d\n",
    "opt['alpha'] = loss_args['alpha']\n",
    "\n",
    "### TR and SCR:\n",
    "opt['penalty_increase_multiplier']=2.    # multiply by..\n",
    "opt['penalty_derease_multiplier']=2.     # divide by..\n",
    "opt['initial_penalty_parameter']=0.01\n",
    "opt['initial_tr_radius']=1\n",
    "opt['successful_treshold']=0.1\n",
    "opt['very_successful_treshold']=0.9\n",
    "\n",
    "opt['grad_tol']=1e-9\n",
    "opt['n_iterations'] = 20\n",
    "\n",
    "# Sampling\n",
    "opt['Hessian_sampling']=True\n",
    "opt['gradient_sampling']=False\n",
    "opt['initial_sample_size_Hessian']=0.025\n",
    "opt['initial_sample_size_gradient']=0.25\n",
    "opt['subproblem_solver']='lanczos'\n",
    "opt['unsuccessful_sample_scaling']=1.5\n",
    "opt['sample_scaling_Hessian']=1\n",
    "opt['sample_scaling_gradient']=1\n",
    "\n",
    "# Subproblem\n",
    "opt['subproblem_solver_SkCR']='exact' # alternatives: exact\n",
    "opt['subproblem_solver_SCR']='exact' # alternatives: lanczos, cauchy_point, exact\n",
    "opt['subproblem_solver_TR']='GLTR' # alternatives: GLTR, cauchy_point, exact, dog_leg, cg\n",
    "\n",
    "opt['solve_each_i-th_krylov_space']=1   \n",
    "opt['krylov_tol']=1e-1\n",
    "opt['exact_tol']=1e-2\n",
    "opt['keep_Q_matrix_in_memory']=True\n",
    "\n",
    "### SGD:\n",
    "opt['n_epochs_sgd']=10\n",
    "opt['learning_rate_sgd']=1e-1 # This SGD implementation expects a constant step size\n",
    "opt['batch_size_sgd']=0.001*n\n",
    "\n",
    "### SAGA:\n",
    "opt['n_epochs_saga']=10\n",
    "opt['learning_rate_saga']=1e-2\n",
    "\n",
    "\n",
    "\n",
    "######################\n",
    "#### Run Methods #####\n",
    "######################\n",
    "\n",
    "SCR=True\n",
    "TR= True\n",
    "SGD= True\n",
    "SAGA= False\n",
    "NS = False\n",
    "SkCR = True\n",
    "LVRG = True\n",
    "SQRN = True\n",
    "\n",
    "if LVRG:\n",
    "    loss_collector=[]\n",
    "    timings_collector=[]\n",
    "    samples_collector=[]\n",
    "\n",
    "    for k in range(n_runs):\n",
    "        LVRG_loss=[]\n",
    "        LVRG_x=[]\n",
    "        (w_Lvrg,_timing, _loss, _samples)=sn.Lvrg(w,loss_computation,gradient_computation,\n",
    "                                                    Hv=hessian_vector_computation,hessian=hessian_computation, X=X, Y=Y, opt=opt, **loss_args)\n",
    "        loss_collector.append(_loss)\n",
    "        timings_collector.append(_timing)\n",
    "        samples_collector.append(_samples)\n",
    "\n",
    "    LVRG_loss = [float(sum(col))/len(col) for col in zip(*loss_collector)]\n",
    "    LVRG_x = [float(sum(col))/len(col) for col in zip(*timings_collector)]\n",
    "    LVRG_samples= [float(sum(col))/len(col) for col in zip(*samples_collector)]\n",
    "\n",
    "if SQRN:\n",
    "    loss_collector=[]\n",
    "    timings_collector=[]\n",
    "    samples_collector=[]\n",
    "\n",
    "    for k in range(n_runs):\n",
    "        SQRN_loss=[]\n",
    "        SQRN_x=[]\n",
    "        (w_Sqrn,_timing, _loss, _samples)=sn.Sqrn(w,loss_computation,gradient_computation,\n",
    "                                                    Hv=hessian_vector_computation,hessian=hessian_computation, X=X, Y=Y, opt=opt, **loss_args)\n",
    "        loss_collector.append(_loss)\n",
    "        timings_collector.append(_timing)\n",
    "        samples_collector.append(_samples)\n",
    "\n",
    "    SQRN_loss = [float(sum(col))/len(col) for col in zip(*loss_collector)]\n",
    "    SQRN_x = [float(sum(col))/len(col) for col in zip(*timings_collector)]\n",
    "    SQRN_samples= [float(sum(col))/len(col) for col in zip(*samples_collector)]\n",
    "\n",
    "\n",
    "if SkCR:\n",
    "    loss_collector=[]\n",
    "    timings_collector=[]\n",
    "    samples_collector=[]\n",
    "\n",
    "    for k in range(n_runs):\n",
    "        SkCR_loss=[]\n",
    "        SkCR_x=[]\n",
    "        (w_SkCR,_timing, _loss, _samples)=sn.SkCR(w,loss_computation,gradient_computation,\n",
    "                                                    Hv=hessian_vector_computation,hessian=hessian_computation, X=X, Y=Y, opt=opt, **loss_args)\n",
    "        loss_collector.append(_loss)\n",
    "        timings_collector.append(_timing)\n",
    "        samples_collector.append(_samples)\n",
    "\n",
    "    SkCR_loss = [float(sum(col))/len(col) for col in zip(*loss_collector)]\n",
    "    SkCR_x = [float(sum(col))/len(col) for col in zip(*timings_collector)]\n",
    "    SkCR_samples= [float(sum(col))/len(col) for col in zip(*samples_collector)]\n",
    "\n",
    "if NS:\n",
    "    loss_collector=[]\n",
    "    timings_collector=[]\n",
    "    samples_collector=[]\n",
    "\n",
    "    for k in range(n_runs):\n",
    "        NS_loss=[]\n",
    "        NS_x=[]\n",
    "        (w_NS,_timing, _loss, _samples)=ns.NS(w,loss_computation,gradient_computation,\n",
    "                                                    Hv=hessian_vector_computation,hessian=hessian_computation, X=X, Y=Y, opt=opt, **loss_args)\n",
    "        loss_collector.append(_loss)\n",
    "        timings_collector.append(_timing)\n",
    "        samples_collector.append(_samples)\n",
    "\n",
    "    NS_loss = [float(sum(col))/len(col) for col in zip(*loss_collector)]\n",
    "    NS_x = [float(sum(col))/len(col) for col in zip(*timings_collector)]\n",
    "    NS_samples= [float(sum(col))/len(col) for col in zip(*samples_collector)]\n",
    "\n",
    "if SCR:\n",
    "    loss_collector=[]\n",
    "    timings_collector=[]\n",
    "    samples_collector=[]\n",
    "    \n",
    "    for k in range(n_runs): \n",
    "        SCR_loss=[]\n",
    "        SCR_x=[]\n",
    "        (w_SCR,_timing, _loss, _samples)=scr.SCR(w,loss_computation,gradient_computation,\n",
    "                                                    Hv=hessian_vector_computation,hessian=hessian_computation, X=X, Y=Y, opt=opt, **loss_args)\n",
    "        loss_collector.append(_loss)\n",
    "        timings_collector.append(_timing)\n",
    "        samples_collector.append(_samples)\n",
    "\n",
    "    SCR_loss = [float(sum(col))/len(col) for col in zip(*loss_collector)]\n",
    "    SCR_x = [float(sum(col))/len(col) for col in zip(*timings_collector)]    \n",
    "    SCR_samples= [float(sum(col))/len(col) for col in zip(*samples_collector)] \n",
    "    \n",
    "if TR:\n",
    "    loss_collector=[]\n",
    "    timings_collector=[]\n",
    "    samples_collector=[]\n",
    "    \n",
    "    for k in range(n_runs): \n",
    "        TR_loss=[]\n",
    "        TR_x=[]\n",
    "        (w_TR,_timing, _loss, _samples)=tr.Trust_Region(w,loss_computation,gradient_computation,\n",
    "                                                    Hv=hessian_vector_computation, hessian=hessian_computation, X=X, Y=Y, opt=opt, **loss_args)\n",
    "        loss_collector.append(_loss)\n",
    "        timings_collector.append(_timing)\n",
    "        samples_collector.append(_samples)\n",
    "\n",
    "    TR_loss = [float(sum(col))/len(col) for col in zip(*loss_collector)]\n",
    "    TR_x = [float(sum(col))/len(col) for col in zip(*timings_collector)]    \n",
    "    TR_samples= [float(sum(col))/len(col) for col in zip(*samples_collector)] \n",
    "    \n",
    "if SGD:\n",
    "    loss_collector=[]\n",
    "    timings_collector=[]\n",
    "    samples_collector=[]\n",
    "    \n",
    "    for k in range(n_runs): \n",
    "        SGD_loss=[]\n",
    "        SGD_x=[]\n",
    "        (w_SGD,_timing, _loss, _samples)=sgd.SGD(w,loss_computation,gradient_computation, X=X, Y=Y, opt=opt, **loss_args)\n",
    "        loss_collector.append(_loss)\n",
    "        timings_collector.append(_timing)\n",
    "        samples_collector.append(_samples)\n",
    "\n",
    "    SGD_loss = [float(sum(col))/len(col) for col in zip(*loss_collector)]\n",
    "    SGD_x = [float(sum(col))/len(col) for col in zip(*timings_collector)]    \n",
    "    SGD_samples= [float(sum(col))/len(col) for col in zip(*samples_collector)] \n",
    "    \n",
    "if SAGA:\n",
    "    loss_collector=[]\n",
    "    timings_collector=[]\n",
    "    samples_collector=[]\n",
    "    \n",
    "    for k in range(n_runs): \n",
    "        SAGA_loss=[]\n",
    "        SAGA_x=[]\n",
    "        (w_SAGA,_timing, _loss, _samples)=saga.SAGA(w,loss_computation,gradient_computation, X=X, Y=Y, opt=opt, **loss_args)\n",
    "        loss_collector.append(_loss)\n",
    "        timings_collector.append(_timing)\n",
    "        samples_collector.append(_samples)\n",
    "\n",
    "    SAGA_loss = [float(sum(col))/len(col) for col in zip(*loss_collector)]\n",
    "    SAGA_x = [float(sum(col))/len(col) for col in zip(*timings_collector)]    \n",
    "    SAGA_samples= [float(sum(col))/len(col) for col in zip(*samples_collector)]     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "show_plots(x_limits_time=(0,2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pycharm-3c69b7d7",
   "language": "python",
   "display_name": "PyCharm (IOE 310)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}